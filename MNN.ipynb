{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/desultir/SolrXLSXResponseWriter/blob/master/MNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "fQvPcbXfGAnt",
        "colab_type": "code",
        "outputId": "87429c44-f829-4a9f-df0a-541194ae4710",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scipy\n",
        "!pip install --upgrade numpy\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: scipy in /usr/local/lib/python3.6/dist-packages (1.2.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.16.2)\n",
            "Requirement already up-to-date: numpy in /usr/local/lib/python3.6/dist-packages (1.16.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GGkj2RPTaJ9g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as pl\n",
        "from ipywidgets import interact, widgets\n",
        "from matplotlib import animation\n",
        "import h5py\n",
        "from google.colab import drive\n",
        "import time\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kf3k_uTGGlqK",
        "colab_type": "code",
        "outputId": "fd332b45-7cf3-4d9f-81a6-cc2ec2ab0330",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive', force_remount=True)\n",
        "with h5py.File('/content/drive/My Drive/Colab Notebooks/Input/train_128.h5','r') as H:\n",
        "  data = np.copy(H['data'])\n",
        "with h5py.File('/content/drive/My Drive/Colab Notebooks/Input/train_label.h5','r') as H:\n",
        "  label = np.copy(H['label'])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rJ4v9obPcy13",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import pickle\n",
        "import glob\n",
        "\n",
        "model_dir = \"/content/drive/My Drive/Colab Notebooks/Models/{}.pk\"\n",
        "\n",
        "def list_models():\n",
        "  return glob.glob(model_dir.format(\"*\"))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DOSt_vG94Mfi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# need to normalize input data to avoid overflow/underflow in initial epochs\n",
        "# normalize each feature independently\n",
        "# options are zscore, minmax\n",
        "def preprocess(input_array, method='zscore'):\n",
        "  if method == 'zscore':\n",
        "    for i in range(input_array.shape[1]):\n",
        "      mean = np.mean(input_array[:, i])\n",
        "      std = np.std(input_array[:, i])\n",
        "      input_array[:, i] = (input_array[:, i] - mean) / std\n",
        "  elif method == 'minmax':\n",
        "    for i in range(input_array.shape[1]):\n",
        "      # range 0 to max\n",
        "      input_array[:, i] = (input_array[:, i] - np.min(input_array[:, i]))\n",
        "      # range 0 to 2\n",
        "      input_array[:, i] /= (np.max(input_array[:, i]) / 2)\n",
        "      # range -1 to 1\n",
        "      input_array[:, i] -= 1\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hCFYVtU06MPR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#use stratified sampling to split train into train/validation\n",
        "#this dataset is actually balanced but still good practice\n",
        "def split(dataset, labels, train_percent=.85):\n",
        "  count = len(dataset)\n",
        "  num_classes = np.max(label) + 1\n",
        "  train = []\n",
        "  train_target = []\n",
        "  validate = []\n",
        "  validate_target = []\n",
        "  for i in range(num_classes):\n",
        "    class_data = np.ravel(np.argwhere(label == i))\n",
        "    np.random.shuffle(class_data)\n",
        "    cutoff = int(len(class_data) * train_percent)\n",
        "    train_idx = class_data[:cutoff]\n",
        "    val_idx = class_data[cutoff:]\n",
        "    train.append(dataset[train_idx])\n",
        "    train_target.append(labels[train_idx])\n",
        "    validate.append(dataset[val_idx])\n",
        "    validate_target.append(labels[val_idx])\n",
        "    \n",
        "  return np.vstack(train), np.hstack(train_target), np.vstack(validate), np.hstack(validate_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RsZmOeyySQq9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#need to one-hot encode labels to map to N output nodes (1 per class)\n",
        "#ie convert each label into a (10,) vector where the relevant column is 1\n",
        "\n",
        "def OHE(input_array, num_classes=10):\n",
        "  output = []\n",
        "  for x in input_array:\n",
        "    output.append(np.zeros((10,)))\n",
        "    output[-1][x] = 1\n",
        "  return np.vstack(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q8KpB-EXchIh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## implemented formulae from here: https://towardsdatascience.com/hyper-parameters-in-action-part-ii-weight-initializers-35aee1a28404\n",
        "class InitWeights(object):\n",
        "  def xavier(self, n_in, n_out, uniform=True):\n",
        "    if uniform:\n",
        "      bounds = np.sqrt(6) / (np.sqrt(n_in + n_out))\n",
        "      return self._uniform(n_in, n_out, bounds) \n",
        "    else:\n",
        "      stddev = np.sqrt(2) / (np.sqrt(n_in + n_out))\n",
        "      return self._truncated_normal(n_in, n_out, stddev)\n",
        "    \n",
        "  def he(self, n_in, n_out, uniform=True):\n",
        "    if uniform:\n",
        "      bounds = np.sqrt(2) / (np.sqrt(n_in))\n",
        "      return self._uniform(n_in, n_out, bounds)      \n",
        "    else:\n",
        "      stddev = np.sqrt(6) / (np.sqrt(n_in))\n",
        "      return self._truncated_normal(n_in, n_out, stddev)\n",
        " \n",
        "  def _uniform(self, n_in, n_out, bounds):\n",
        "    W = np.random.uniform(\n",
        "        low=-bounds,\n",
        "        high=bounds,\n",
        "        size=(n_in, n_out)\n",
        "      )\n",
        "    return W\n",
        "  \n",
        "  def _truncated_normal(self, n_in, n_out, stddev):\n",
        "    W = np.random.normal(\n",
        "        loc=0,\n",
        "        scale=stddev,\n",
        "        size=(n_in, n_out)\n",
        "      )\n",
        "    #truncate results - anything > 2 stddev out gets clipped\n",
        "    W[W> 2*stddev] = 2*stddev\n",
        "    W[W<-2*stddev] = -2*stddev\n",
        "    return W\n",
        "  \n",
        "  def __init__(self, init_method=\"xavier\"):\n",
        "    if init_method==\"xavier\":\n",
        "      self.f = self.xavier\n",
        "    elif init_method==\"he\":\n",
        "      self.f = self.he"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "coc2QCMsn63E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def calc_MSE(y, y_hat):\n",
        "  error = y-y_hat\n",
        "  return np.mean(np.sum(error**2, axis=1))\n",
        "\n",
        "def labels_from_preds(preds):\n",
        "  return np.argmax(preds, axis=1)\n",
        "\n",
        "def calc_accuracy(labels, target):\n",
        "  return np.sum(labels == target) / len(target)\n",
        "\n",
        "#wasn't sure if we could use a package to shuffle so found this code: https://stackoverflow.com/questions/4601373/better-way-to-shuffle-two-numpy-arrays-in-unison\n",
        "def shuffle_in_unison(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    shuffled_a = np.empty(a.shape, dtype=a.dtype)\n",
        "    shuffled_b = np.empty(b.shape, dtype=b.dtype)\n",
        "    permutation = np.random.permutation(len(a))\n",
        "    for old_index, new_index in enumerate(permutation):\n",
        "        shuffled_a[new_index] = a[old_index]\n",
        "        shuffled_b[new_index] = b[old_index]\n",
        "    return shuffled_a, shuffled_b\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JbRDaYgyBsh8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The function for ReLU\n",
        "\n",
        "\n",
        "$ f(x) = \\begin{cases}\n",
        "    x & \\mbox{if } x > 0 \\\\\n",
        "    0 & \\mbox{otherwise}\n",
        "\\end{cases}$\n",
        "\n",
        "The function for ReLU's derivative\n",
        "\n",
        "$ f(x) = \\begin{cases}\n",
        "    1 & \\mbox{if } x > 0 \\\\\n",
        "    0 & \\mbox{otherwise}\n",
        "\\end{cases}$\n",
        "\n",
        "\n",
        "The function for Leaky ReLU\n",
        "\n",
        "\n",
        "$ f(x) = \\begin{cases}\n",
        "    x & \\mbox{if } x > 0 \\\\\n",
        "    0.01x & \\mbox{otherwise}\n",
        "\\end{cases}$\n",
        "\n",
        "The function for ReLU's derivative\n",
        "\n",
        "$ f(x) = \\begin{cases}\n",
        "    1 & \\mbox{if } x > 0 \\\\\n",
        "    0.01 & \\mbox{otherwise}\n",
        "\\end{cases}$\n"
      ]
    },
    {
      "metadata": {
        "id": "kYXDLyEWGG2r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "class Activation(object):\n",
        "    def tanh(self, x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def tanh_deriv(self, a):\n",
        "        # a = np.tanh(x)   \n",
        "        return 1.0 - a**2\n",
        "    def logistic(self, x):\n",
        "        return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "    def logistic_deriv(self, a):\n",
        "        # a = logistic(x) \n",
        "        return  a * (1 - a )\n",
        "      \n",
        "    def ReLU(self, x):\n",
        "        x[x<0] =0\n",
        "        return x\n",
        "      \n",
        "    def ReLU_deriv(self, a):\n",
        "        der = np.zeros(a.shape)\n",
        "        der[a>0] =1\n",
        "        return der\n",
        "      \n",
        "    def leaky_ReLU(self, x):\n",
        "        x = np.where(x > 0, x, x*0.01)\n",
        "        return x\n",
        "      \n",
        "    def leaky_ReLU_deriv(self, a):\n",
        "        der = np.full(a.shape, 0.01)\n",
        "        der[a>0] =1\n",
        "        return der\n",
        "      \n",
        "    def softmax(self, x):\n",
        "        # apply max normalization to avoid overflow\n",
        "        x_norm = (x.T - np.max(x, axis=1)).T\n",
        "        return softmax(x_norm, axis=1)\n",
        "      \n",
        "    def softmax_deriv(self, a):\n",
        "        return np.ones(a.shape)\n",
        "    \n",
        "    def __init__(self,activation='tanh'):\n",
        "        if activation == 'logistic':\n",
        "            self.f = self.logistic\n",
        "            self.f_deriv = self.logistic_deriv\n",
        "        elif activation == 'tanh':\n",
        "            self.f = self.tanh\n",
        "            self.f_deriv = self.tanh_deriv\n",
        "        elif activation == 'relu':\n",
        "            self.f = self.ReLU\n",
        "            self.f_deriv = self.ReLU_deriv\n",
        "        elif activation == 'leaky_relu':\n",
        "            self.f = self.leaky_ReLU\n",
        "            self.f_deriv = self.leaky_ReLU_deriv\n",
        "        elif activation == 'softmax':\n",
        "            self.f = self.softmax\n",
        "            self.f_deriv = self.softmax_deriv\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HbjWqZ24L3Ot",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Loss_function(object):\n",
        "    def MSE(self, y, y_hat):\n",
        "        error = y-y_hat\n",
        "        loss=np.sum(error**2)\n",
        "        return loss\n",
        "      \n",
        "    def Cross_entropy(self, y, y_hat):\n",
        "        return -np.log(y_hat[np.argmax(y)])\n",
        "      \n",
        "    def l2_reg(self, reg_weight, layers, sample_weight):\n",
        "        accum = 0\n",
        "        for layer in layers:\n",
        "          accum += np.sum(np.square(layer.W))\n",
        "          \n",
        "        return accum*reg_weight*sample_weight/2\n",
        "        \n",
        "    def __init__(self,loss='cross_entropy'):\n",
        "        if loss == 'MSE':\n",
        "            self.loss = self.MSE\n",
        "        elif loss == 'cross_entropy':\n",
        "            self.loss = self.Cross_entropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BeoakGoCGLvb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "class HiddenLayer(object):    \n",
        "    def __init__(self,n_in, n_out,\n",
        "                 activation_last_layer='tanh',activation='tanh', W=None, b=None,\n",
        "                init_uniform=True, weight_decay=None):\n",
        "        \"\"\"\n",
        "        Typical hidden layer of a MLP: units are fully-connected and have\n",
        "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
        "        and the bias vector b is of shape (n_out,).\n",
        "\n",
        "        NOTE : The nonlinearity used here is tanh\n",
        "\n",
        "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
        "\n",
        "        :type n_in: int\n",
        "        :param n_in: dimensionality of input\n",
        "\n",
        "        :type n_out: int\n",
        "        :param n_out: number of hidden units\n",
        "\n",
        "        :type activation: string\n",
        "        :param activation: Non linearity to be applied in the hidden\n",
        "                           layer\n",
        "        :type init_uniform: bool\n",
        "        :param init_uniform: Whether to draw init weights from uniform dist (else normal)\n",
        "        \n",
        "        :type weight_decay: float/None/False\n",
        "        :param weight_decay: Weight to apply to l2 reg loss factor (else none/false)\n",
        "        \"\"\"\n",
        "        self.input=None\n",
        "        self.activation=Activation(activation).f\n",
        "        \n",
        "        if activation == 'relu':\n",
        "          self.init_weights = InitWeights(\"he\").f\n",
        "        else:\n",
        "          self.init_weights = InitWeights(\"xavier\").f\n",
        "        \n",
        "        # activation deriv of last layer\n",
        "        self.activation_deriv=None\n",
        "        if activation_last_layer:\n",
        "            self.activation_deriv=Activation(activation_last_layer).f_deriv\n",
        "\n",
        "        if W is not None:\n",
        "          self.W = W\n",
        "        else:\n",
        "          self.W = self.init_weights(n_in, n_out, init_uniform)\n",
        "\n",
        "        if b is not None:\n",
        "          self.b = b\n",
        "        else:\n",
        "          self.b = np.zeros(n_out,)  \n",
        "          \n",
        "        self.weight_decay = weight_decay\n",
        "          \n",
        "        self.grad_W = np.zeros(self.W.shape)\n",
        "        self.grad_b = np.zeros(self.b.shape)\n",
        "        \n",
        "        # create arrays to store the velocity values for momentum calculation\n",
        "        self.vW = np.zeros(self.W.shape)\n",
        "        self.vb = np.zeros(self.b.shape)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        '''\n",
        "        :type input: numpy.array\n",
        "        :param input: a symbolic tensor of shape (n_in,)\n",
        "        '''\n",
        "        lin_output = np.dot(input, self.W) + self.b\n",
        "        self.output = (\n",
        "            lin_output if self.activation is None\n",
        "            else self.activation(lin_output)\n",
        "        )\n",
        "        self.input=input\n",
        "        return self.output\n",
        "    \n",
        "    def backward(self, delta, output_layer=False, sampleweight=1):\n",
        "        self.grad_W = np.atleast_2d(self.input).T.dot(np.atleast_2d(delta))\n",
        "        \n",
        "        if self.weight_decay:\n",
        "          self.grad_W += self.W * self.weight_decay * sampleweight\n",
        "        self.grad_b = np.sum(delta, axis=0)\n",
        "        if self.activation_deriv:\n",
        "            delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
        "        return delta\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_LKgiGhdGQbS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "class MLP:\n",
        "    \"\"\"\n",
        "    \"\"\"      \n",
        "    def __init__(self, layers=None, activation=[None,'tanh','tanh'], init_uniform=True, weight_decay=False, from_file=None):\n",
        "        \"\"\"\n",
        "        :param layers: A list containing the number of units in each layer.\n",
        "        Should be at least two values\n",
        "        :param activation: The activation function to be used. Can be\n",
        "        \"logistic\" or \"tanh\"\n",
        "        :param init_uniform: Whether to draw init weights from uniform dist (else normal)\n",
        "        :param weight_decay: lambda for strength of l2 regularization on weights (else False/None for no reg)\n",
        "        :param from_file: a file to load to get pretrained weights. \n",
        "        \"\"\"        \n",
        "        ### initialize layers\n",
        "        self.layers=[]\n",
        "        self.params= {'activation':activation, 'layers':layers, 'weight_decay': weight_decay, 'init_uniform': init_uniform}\n",
        "        \n",
        "        if from_file:\n",
        "          dumped_model = self._load_model(from_file)\n",
        "          self.params = dumped_model['params']\n",
        "          self.activation=self.params['activation']\n",
        "          layers = self.params['layers']\n",
        "          init_uniform = self.params['init_uniform']\n",
        "          for i in range(len(self.params['layers'])-1):\n",
        "              self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1],  \n",
        "                                             W=dumped_model['weights'][i][0], b=dumped_model['weights'][i][1], weight_decay=weight_decay, init_uniform=init_uniform))\n",
        "        else:\n",
        "          self.activation=activation\n",
        "          for i in range(len(layers)-1):\n",
        "              self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1], weight_decay=weight_decay, init_uniform=init_uniform))\n",
        "            \n",
        "    def forward(self,input):\n",
        "        for layer in self.layers:\n",
        "            output=layer.forward(input)\n",
        "            input=output\n",
        "        return output\n",
        "      \n",
        "    def calculate_loss(self,y,y_hat):\n",
        "        activation_deriv=Activation(self.activation[-1]).f_deriv\n",
        "        # call to loss function\n",
        "        loss=[]\n",
        "        delta=[]\n",
        "\n",
        "        for i, single_y in enumerate(y):\n",
        "          loss.append(Loss_function('MSE').loss(single_y, y_hat[i]))\n",
        "          error = single_y-y_hat[i]\n",
        "        # calculate the delta of the output layer\n",
        "          delta.append(np.array(-error*activation_deriv(y_hat[i])))\n",
        "        # return loss and delta\n",
        "        loss = np.array(loss)\n",
        "        if self.params['weight_decay']:\n",
        "          loss += Loss_function().l2_reg(self.params['weight_decay'], self.layers, len(y)/self.Xcount)\n",
        "\n",
        "        return loss,np.array(delta)\n",
        "        \n",
        "    def backward(self,delta, sampleweight):\n",
        "        delta=self.layers[-1].backward(delta,output_layer=True, sampleweight=sampleweight)\n",
        "        for layer in reversed(self.layers[:-1]):\n",
        "            delta=layer.backward(delta, sampleweight=sampleweight)\n",
        "            \n",
        "    def update(self,lr):\n",
        "        for layer in self.layers:\n",
        "            layer.W -= lr * layer.grad_W\n",
        "            layer.b -= lr * layer.grad_b\n",
        "            \n",
        "    def update_momentum(self, lr, mom):\n",
        "        for layer in self.layers:\n",
        "            layer.vW = mom * layer.vW + lr * layer.grad_W\n",
        "            layer.vb = mom * layer.vb + lr * layer.grad_b\n",
        "            layer.W -= layer.vW\n",
        "            layer.b -= layer.vb        \n",
        "\n",
        "    def fit(self,X,y,learning_rate=0.1, epochs=100):\n",
        "        \"\"\"\n",
        "        Online learning.\n",
        "        :param X: Input data or features\n",
        "        :param y: Input targets\n",
        "        :param learning_rate: parameters defining the speed of learning\n",
        "        :param epochs: number of times the dataset is presented to the network for learning\n",
        "        \"\"\" \n",
        "        X=np.array(X)\n",
        "        y=np.array(y)\n",
        "        to_return = np.zeros(epochs)\n",
        "        self.Xcount = len(X)\n",
        "        for k in range(epochs):\n",
        "            #print('epoch', k)\n",
        "            loss=np.zeros(X.shape[0])\n",
        "            for it in range(X.shape[0]):\n",
        "                i=np.random.randint(X.shape[0])\n",
        "                \n",
        "                # forward pass\n",
        "                y_hat = self.forward(X[i])\n",
        "                # backward pass\n",
        "                loss[it],delta=self.calculate_loss([y[i]],[y_hat])\n",
        "                self.backward(delta, 1/self.Xcount)\n",
        "                # update\n",
        "                self.update(learning_rate)\n",
        "            to_return[k] = np.mean(loss)\n",
        "            if not k % 10:\n",
        "              print(\".\", end=\"\")\n",
        "        return to_return\n",
        "      \n",
        "    def fit_mb(self,X,y,mini_batch_size,learning_rate=0.1, epochs=100):\n",
        "        \"\"\"\n",
        "        Online learning.\n",
        "        :param X: Input data or features\n",
        "        :param y: Input targets\n",
        "        :param learning_rate: parameters defining the speed of learning\n",
        "        :param epochs: number of times the dataset is presented to the network for learning\n",
        "        \"\"\" \n",
        "        X=np.array(X)\n",
        "        y=np.array(y)\n",
        "        to_return = np.zeros(epochs) #array to store values of mean loss for each epoch for plotting later\n",
        "        self.Xcount = len(X)\n",
        " \n",
        "        for k in range(epochs): #for each epoch\n",
        "            X, y = shuffle_in_unison(X, y) #shuffle the input data and input targets\n",
        "            loss=np.zeros(X.shape[0]) #create array of zeros whose lengths = #samples.\n",
        "            \n",
        "            #partition training data (X, y) into mini-batches\n",
        "            for j in range(0, X.shape[0], mini_batch_size):\n",
        "              X_mini = X[j:j + mini_batch_size]\n",
        "              y_mini = y[j:j + mini_batch_size]\n",
        "              # forward pass\n",
        "              y_hat = self.forward(X_mini) #forward feed the mini_batches to get outputs (y_hat)\n",
        "              \n",
        "              # backwards pass\n",
        "              loss[j:j + mini_batch_size], delta=self.calculate_loss(y[j:j + mini_batch_size], y_hat) #input y and y_hat into calculate_loss. Output = loss and delta\n",
        "              self.backward(delta, mini_batch_size/self.Xcount) #pass delta from calculate_loss to backward.\n",
        "\n",
        "              # update\n",
        "              self.update(learning_rate)\n",
        "            to_return[k] = np.mean(loss) #add mean loss to to_return\n",
        "            if not k % 10:\n",
        "              print(\".\", end=\"\")\n",
        "        return to_return\n",
        "      \n",
        "    def fit_SGD_momentum(self,X,y,learning_rate=0.1, epochs=100, momentum=0.9):\n",
        "        \"\"\"\n",
        "        Online learning.\n",
        "        :param X: Input data or features\n",
        "        :param y: Input targets\n",
        "        :param learning_rate: parameters defining the speed of learning\n",
        "        :param epochs: number of times the dataset is presented to the network for learning\n",
        "        \"\"\" \n",
        "        X=np.array(X)\n",
        "        y=np.array(y)\n",
        "        to_return = np.zeros(epochs)\n",
        "        self.Xcount = len(X)\n",
        "        for k in range(epochs):\n",
        "            loss=np.zeros(X.shape[0])\n",
        "            \n",
        "            # loop through training examples\n",
        "            for j in range(X.shape[0]):\n",
        "              i=np.random.randint(X.shape[0])\n",
        "                \n",
        "              # forward pass\n",
        "              y_hat = self.forward(X[i])\n",
        "                \n",
        "              # backward pass\n",
        "              loss[j],delta=self.calculate_loss([y[i]],[y_hat])\n",
        "              self.backward(delta, X.shape[0]/self.Xcount)\n",
        "                \n",
        "              # update\n",
        "              self.update_momentum(learning_rate, momentum)\n",
        "            to_return[k] = np.mean(loss)\n",
        "            if not k % 10:\n",
        "              print(\".\", end=\"\")\n",
        "        return to_return  \n",
        "\n",
        "    def predict(self, x):\n",
        "        x = np.array(x)\n",
        "        output = []\n",
        "        for i in np.arange(x.shape[0]):\n",
        "            output.append(self.forward(x[i,:]))\n",
        "        return np.vstack(output)\n",
        "      \n",
        "\n",
        "    def save_model(self, name):\n",
        "      model = {'params':self.params, 'weights':[]}\n",
        "      for x in self.layers:\n",
        "        model['weights'].append((x.W, x.b))\n",
        "        \n",
        "      with open(model_dir.format(name), 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "        \n",
        "    def _load_model(self, name):\n",
        "      with open(model_dir.format(name), 'rb') as f:\n",
        "        return pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DOPAt_9nGd5y",
        "colab_type": "code",
        "outputId": "56368d7e-e0e4-4509-cd92-8a35dbea7091",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "np.seterr(all=\"warn\")\n",
        "np.random.seed(1)\n",
        "procdata = np.copy(data)\n",
        "preprocess(procdata, 'zscore')\n",
        "\n",
        "#split data\n",
        "train, train_target, validate, validate_target = split(data, label)\n",
        "#one hot encode targets\n",
        "train_target = OHE(train_target, 10)\n",
        "validate_target = OHE(validate_target, 10)\n",
        "second_layer = False\n",
        "relu = True\n",
        "if second_layer:\n",
        "  nn = MLP([128,60,30,10], [None,'logistic','logistic','tanh'])\n",
        "elif relu:\n",
        "  nn = MLP([128,60,10, 10], [None, 'relu', 'relu', 'softmax'], False)\n",
        "  start = time.time()\n",
        "  MSE = nn.fit_mb(train, train_target, learning_rate=0.01, epochs=500, mini_batch_size=32)\n",
        "  print(\"{}s to train\".format(time.time() - start))\n",
        "else:\n",
        "  nn = MLP([128,60,10], [None,'logistic','tanh'], init_uniform=False, weight_decay=0.5)\n",
        "  start = time.time()\n",
        "  MSE = nn.fit_mb(train, train_target, learning_rate=0.01, epochs=500, mini_batch_size=32)\n",
        "  print(\"{}s to train\".format(time.time() - start))\n",
        "print('loss:%f'%MSE[-1])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/special/_logsumexp.py:112: RuntimeWarning: underflow encountered in exp\n",
            "  tmp = np.exp(a - a_max)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/special/_logsumexp.py:215: RuntimeWarning: underflow encountered in exp\n",
            "  return np.exp(x - logsumexp(x, axis=axis, keepdims=True))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: underflow encountered in square\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:64: RuntimeWarning: underflow encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ".8.548265218734741s to train\n",
            "loss:0.900421\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-U-Uf7i9YX9t",
        "colab_type": "code",
        "outputId": "02b0a718-de6a-40e3-a557-7de5437c7215",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "cell_type": "code",
      "source": [
        "pl.figure(figsize=(15,4))\n",
        "pl.plot(MSE[:2])\n",
        "pl.grid()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3cAAAD8CAYAAADUrF2QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1080x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "ms28tiW02uzz",
        "colab_type": "code",
        "outputId": "710478b7-0cd4-4933-e705-23755845bbdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "#get validation score\n",
        "#nn = load_model(\"tdm1\")\n",
        "preds = nn.predict(validate)\n",
        "\n",
        "loss = calc_MSE(preds, validate_target)\n",
        "loss"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: overflow encountered in exp\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: underflow encountered in exp\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: underflow encountered in true_divide\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.43496945947156873"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "Chank-E-im1p",
        "colab_type": "code",
        "outputId": "fc49bcf4-e3b4-4b1e-f221-d3fa82684856",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "nn.save_model(\"weight.1\")\n",
        "\n",
        "calc_accuracy(labels_from_preds(preds), labels_from_preds(validate_target)) # we have to de-OHE the predictions and the target data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.829"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "VPIf-BuaBzlC",
        "colab_type": "code",
        "outputId": "f75f36be-04d6-4221-bcc3-62eed660c498",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "nn2 = MLP(from_file=\"tdm1\")\n",
        "nn2.predict(validate)\n",
        "loss = calc_MSE(preds, validate_target)\n",
        "loss\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4986497202072747"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "CauivsamljjX",
        "colab_type": "code",
        "outputId": "8d55cb78-529e-4374-a9f6-8b23ad9b453b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "cell_type": "code",
      "source": [
        "np.seterr(all=\"warn\")\n",
        "np.random.seed(1)\n",
        "procdata = np.copy(data)\n",
        "preprocess(procdata, 'zscore')\n",
        "\n",
        "#split data\n",
        "train, train_target, validate, validate_target = split(data, label)\n",
        "#one hot encode targets\n",
        "train_target = OHE(train_target, 10)\n",
        "validate_target = OHE(validate_target, 10)\n",
        "second_layer = False\n",
        "relu = False\n",
        "if second_layer:\n",
        "  nn = MLP([128,60,30,10], [None,'logistic','logistic','tanh'])\n",
        "elif relu:\n",
        "  nn = MLP([128,60,10], [None, 'relu', 'relu'], False)\n",
        "  start = time.time()\n",
        "  MSE = nn.fit_SGD_momentum(train, train_target, learning_rate=0.001, epochs=25)\n",
        "  print(\"{}s to train\".format(time.time() - start))\n",
        "else:\n",
        "  nn = MLP([128,60,10], [None,'logistic','tanh'], False)\n",
        "  start = time.time()\n",
        "  MSE = nn.fit_SGD_momentum(train, train_target, learning_rate=0.01, epochs=25)\n",
        "  print(\"{}s to train\".format(time.time() - start))\n",
        "print('loss:%f'%MSE[-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: overflow encountered in exp\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: underflow encountered in exp\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: underflow encountered in true_divide\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:82: RuntimeWarning: underflow encountered in multiply\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: RuntimeWarning: underflow encountered in multiply\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: RuntimeWarning: underflow encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "...197.74567532539368s to train\n",
            "loss:2.176979\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XAmMCUzSupSo",
        "colab_type": "code",
        "outputId": "dc504048-9388-4cd2-904a-e54dc3b7f1bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "cell_type": "code",
      "source": [
        "pl.figure(figsize=(15,4))\n",
        "pl.plot(MSE)\n",
        "pl.grid()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2gAAAD4CAYAAACZmMXjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl0VfWh/v/nDJlOZjKHIYQpAwgo\nMwoIAs4CamsdSmtbi7XWa7u67q+3t1d7p7aX2n4Xeqsi6LVW721s6oCtrYgMIjPIlBBCwhDIPM8n\nJ2fYvz8CEWQKkGSfJO/XWlkke5/hiR425zmfz/5si2EYhgAAAAAAprOaHQAAAAAA0IGCBgAAAAB+\ngoIGAAAAAH6CggYAAAAAfoKCBgAAAAB+wt7bT1hV1dTbTwkAAAAAfiMuLvyi+xhBAwAAAAA/QUED\nAAAAAD9BQQMAAAAAP0FBAwAAAAA/0aVFQpYvX649e/bI4/Fo2bJlWrhwYee+srIy/ehHP5Lb7VZm\nZqb+7d/+rcfCAgAAAEB/dtkRtO3bt6ugoEBZWVlavXq1fvGLX5yz/1e/+pW+9a1vKTs7WzabTaWl\npT0WFgAAAAD6M4thGMalbuD1euVyueRwOOT1ejVz5kxt3bpVNptNPp9Ps2fP1qZNm2Sz2br0hCyz\nDwAAAGAgu6Zl9m02mxwOhyQpOztbs2fP7ixjtbW1Cg0N1S9/+Us9+OCD+s1vftNNkQEAAABg4Ony\nIiHr1q1Tdna2nnnmmc5thmGooqJCS5cu1ZtvvqlDhw5p48aNPZGzRx0va9SaLcfl8frMjgIAAABg\nAOtSQdu8ebNefvllrVq1SuHhXwzHRUdHKzk5WcOGDZPNZtOMGTNUUFDQY2F7yv7Car23+bje2XTM\n7CgAAAAABrDLFrSmpiYtX75cK1euVFRU1Dn77Ha7hg4dqhMnTkiScnNzlZqa2iNBe9KtU4cpYZBD\nf995Unvyq8yOAwAAAGCAuuwiIVlZWXrhhRfOKV7Tpk1TWlqaFixYoKKiIv3kJz+RYRgaM2aMfv7z\nn8tqvXjv89dFQoqrmvUfb+yWzWrRM9+cooRoh9mRAAAAAPRDl1ok5LIFrbv5a0GTpK05ZVr9lzwN\njQ/TP399kgIDurYyJQAAAAB01TWt4jiQzByXpJsnJutUZbPe/PiI2XEAAAAADDAUtC95cP5opSSG\n67MDZdq8n4tuAwAAAOg9FLQvCbDb9MTicQoNtuvNj4/oZIX/TskEAAAA0L9Q0C4gLipE37krU26P\nTy++m6PWNrfZkQAAAAAMABS0i5gwKlZ3zkhRZb1Tr/41T728lgoAAACAAYiCdgmLZ6UqfViU9hZU\n6+87T5odBwAAAEA/R0G7BJvVqmWLxikyLFB/3nhM+SfrzI4EAAAAoB+joF1GZGigvrdonCTp5fdz\n1dDsMjkRAAAAgP6KgtYFY4ZG6f6bR6qhpV0r1+TK6/OZHQkAAABAP0RB66Jbpw7VDWPidPhkvd79\n9LjZcQAAAAD0QxS0LrJYLPrWHRmKjw7Rh9uLtLegyuxIAAAAAPoZCtoVcATb9cTicQqwW/XqX/JU\nWe80OxIAAACAfoSCdoWGJYTr6wvT1Ory6KV3c+T2eM2OBAAAAKCfoKBdhZvGJ2nW+CQVVTTpf9cV\nmB0HAAAAQD9BQbtKDy8Yo2HxYdq0r1RbDpaZHQcAAABAP0BBu0qBATY9sWScQoLs+sNH+SqubDY7\nEgAAAIA+joJ2DeKjHfrOnRlq9/j0u3cPyunymB0JAAAAQB9GQbtG14+J0+3ThqmizqnXPsyTYRhm\nRwIAAADQR1HQusG9c0ZozNAo7cmv0se7TpkdBwAAAEAfRUHrBjarVY8vGquI0ED9aeNRFRTXmx0J\nAAAAQB9EQesmUWFB+t6isfIZhl56L0eNLe1mRwIAAADQx1DQulHasGjdN2ek6pvbtXJNrnw+zkcD\nAAAA0HUUtG52+7RhmjgqVnlFdXrvs+NmxwEAAADQh1DQupnFYtF37spQbGSw/rL1hA4crTY7EgAA\nAIA+goLWAxzBAfr+kutkt1m16oNDqm5wmh0JAAAAQB9AQeshKYnhemThGLW0efTiuzlye3xmRwIA\nAADg5yhoPWjW+CTdOC5RJ8qb9MdPCsyOAwAAAMDPUdB6kMVi0SO3pmlIXJg27C3RttxysyMBAAAA\n8GMUtB4WFGDT95eMU3CgTb//+2GVVDWbHQkAAACAn6Kg9YKEQQ59+84Mtbt9+t27OXK6PGZHAgAA\nAOCHKGi9ZFJavBZOGary2lb9/u+HZRhcxBoAAADAuShovej+m0dq1JBI7cyr1Cd7is2OAwAAAMDP\nUNB6kd1m1fcWjVOEI0BZ6wt1tKTB7EgAAAAA/IjF6MJcu+XLl2vPnj3yeDxatmyZFi5c2Llv3rx5\nSkxMlM1mkyQ999xzSkhIuOhjVVU1dUPsvu3QiVr9JmufosOD9Ow3pyjcEWh2JAAAAAC9JC4u/KL7\n7Je78/bt21VQUKCsrCzV1dVpyZIl5xQ0SVq1apVCQ0OvPekAkTl8kBbPGqF3Pz2mVz44pB9+ZYKs\nVovZsQAAAACY7LIFbcqUKRo/frwkKSIiQk6nU16vt3PEDFfnzhkpOlrSoANHa/TB1hNadFOq2ZEA\nAAAAmOyy56DZbDY5HA5JUnZ2tmbPnn1eOXv22Wf14IMP6rnnnmN1wi6yWiz6zl2ZiokI1prPjmv9\n58U6cqpepdUtamxpl9fnMzsiAAAAgF7WpXPQJGndunVauXKlXnvtNYWHfzFn8r333tOsWbMUGRmp\n73//+1qyZIluu+22iz4O56Cd63hZo3755h55vOf/b3AE2RUWEqAwR4DCQgIUGhygcEeAQkM6fg4P\n+eL7M18BdtZ9AQAAAPzZpc5B61JB27x5s1asWKHVq1crKirqord76623VFNTo6eeeuqit6Ggna+o\nvEmHT9ap2elWi9OtptN/Np/1/YUK3IUEBdjOKmx2hTkCFRYcoNAQu8IdgR1/hgR2FL7T3wcGWGWx\ncA4cAAAA0BuuaZGQpqYmLV++XK+//vp55aypqUlPP/20XnrpJQUGBmrXrl269dZbrz3xAJOSGK6U\nxIv/TzIMQ23t3o7S1uZWc2tHebvYV4vTrbLaFrW7uzZNMjDAqm/elq7pYxO761cCAAAAcBUuW9A+\n/PBD1dXV6emnn+7cNm3aNKWlpWnBggWaPXu2HnjgAQUFBSkzM/OS0xtxdSwWi0KC7AoJsitWIV2+\nn9vjVbPTo6bW9tPlzqPm1vbTRc6jZme7mp0e5RXV6r3PjmtqZoKsjKQBAAAApunyOWjdhSmO/ue1\nD/P02YEyPXX/eE0cFWt2HAAAAKBfu9QUR1aUgBZMHipJWrf7lMlJAAAAgIGNggYNjQ9T+rAoHTpR\np+KqZrPjAAAAAAMWBQ2Szh5FKzY5CQAAADBwUdAgSZowKlZxUcHalluuptZ2s+MAAAAAAxIFDZIk\nq9Wi+ZOGyu3xadO+UrPjAAAAAAMSBQ2dbhqfpOBAm9Z/XiyPt2vXUAMAAADQfSho6BQSZNdN45NU\n39yu3fmVZscBAAAABhwKGs4xf9IQWcRiIQAAAIAZKGg4R3y0QxNGxepYaaOOljSYHQcAAAAYUCho\nOM+CKR1L7n/MhasBAACAXkVBw3nSh0VpSFyYdh+uUm1jm9lxAAAAgAGDgobzWCwWLZg8RD7D0PrP\nS8yOAwAAAAwYFDRc0PSxCQoLCdCmfSVyub1mxwEAAAAGBAoaLijAbtPN1w9WS5tH23LLzY4DAAAA\nDAgUNFzU3OsHy2a1aN3uYhmGYXYcAAAAoN+joOGiosODNDUjXqXVLTp0os7sOAAAAEC/R0HDJc2f\nzJL7AAAAQG+hoOGSUpMiNGpIpA4crVFZTYvZcQAAAIB+jYKGy1pwehTtkz3FJicBAAAA+jcKGi7r\nhjGxGhQRpC0Hy9Xa5jY7DgAAANBvUdBwWTarVbdMGiKX26tP95eZHQcAAADotyho6JLZE5IVGGDV\nJ3uK5fX5zI4DAAAA9EsUNHRJaHCAbhyXpJrGNu09Um12HAAAAKBfoqChy+ZPHiJJWseS+wAAAECP\noKChy5JiQjVuxCAdKW5QUXmT2XEAAACAfoeChiuykAtXAwAAAD2GgoYrMjZ1kJJiHNpxqEINzS6z\n4wAAAAD9CgUNV8RisWj+5KHy+gxt2FtidhwAAACgX6Gg4YrNHJuo0GC7Nu4tkdvjNTsOAAAA0G9Q\n0HDFggJtmj0hWY2tbu04VGl2HAAAAKDfoKDhqsy7YYisFovW7T4lwzDMjgMAAAD0CxQ0XJWYyGBN\nSovTycpmHTlVb3YcAAAAoF+goOGqLTi95P7aXSy5DwAAAHQHChqu2sjBEUpNCte+gmpV1jvNjgMA\nAAD0eV0qaMuXL9cDDzyg++67T2vXrr3gbX7zm9/o61//ereGg387s+S+IWn9nmKz4wAAAAB93mUL\n2vbt21VQUKCsrCytXr1av/jFL867TWFhoXbt2tUjAeHfpqTHKzIsUJsPlMrp8pgdBwAAAOjTLlvQ\npkyZohUrVkiSIiIi5HQ65fWee+2rX/3qV/rhD3/YMwnh1+w2q+bdMEROl1dbDpaZHQcAAADo0y5b\n0Gw2mxwOhyQpOztbs2fPls1m69z/zjvvaOrUqRo8eHDPpYRfmzMxWXabVev2FMvHkvsAAADAVevy\nIiHr1q1Tdna2nnnmmc5t9fX1euedd/Too4/2SDj0DRGOQM0Ym6DKOqcOHK0xOw4AAADQZ3WpoG3e\nvFkvv/yyVq1apfDw8M7t27dvV21trR5++GE9+eSTys3NveA5auj/ziy5/zFL7gMAAABXzWIYl56T\n1tTUpIceekivv/66YmJiLnq74uJi/dM//ZP+8Ic/XPIJq6qari4p/N6v/2+v8orq9G/fmqoh8WFm\nxwEAAAD8Ulxc+EX32S935w8//FB1dXV6+umnO7dNmzZNaWlpWrBgQfckRL+wYPJQ5RXVad2eU/rm\n7RlmxwEAAAD6nMuOoHU3RtD6L59h6Kcrt6uu2aXnnpipcEeg2ZEAAAAAv3OpEbQuLxICXI7VYtEt\nk4fI7fFp075Ss+MAAAAAfQ4FDd3qpuuSFBJk0/rPi+Xx+syOAwAAAPQpFDR0q5Agu266Lln1ze3a\nfbjS7DgAAABAn0JBQ7e7ZfIQWSR9vPuUevkURwAAAKBPo6Ch28VHhWji6FgdL2vS0dJGs+MAAAAA\nfQYFDT3izIWr1+3mwtUAAABAV1HQ0CPShkVpaHyYdh+uUm1jm9lxAAAAgD6BgoYeYbFYNH/yEPkM\nQ598Xmx2HAAAAKBPoKChx0zPTFC4I0Cf7iuVy+01Ow4AAADg9yho6DEBdpvmXj9YLW0ebcspNzsO\nAAAA4PcoaOhRc68fLJvVwpL7AAAAQBdQ0NCjIsOCNDUjQWU1rco9UWt2HAAAAMCvUdDQ4xZMGSJJ\n+ngXi4WczeP16cipejW0tJsdBQAAAH7CbnYA9H/DEyM0ekikDh6rUVlNi5JiQs2OZBqfYajgVL12\nHKrQrsOVamnzKDDAqtunpei2qcMUFGgzOyIAAABMZDF6+cSgqqqm3nw6+Indhyv14ns5mnvDYH19\nYZrZcXqVYRg6WdGsHYcqtCOvQnVNLklSZGigJoyK0f7CGjW0tCsqLFBLZo/QjeOSZLVaTE4NAACA\nnhIXF37RfRQ09Aqvz6efvLxNTU63fvP9GxUaHGB2pB5XUdfaUcoOVaisplWSFBJk16S0OE3PTFD6\nsGhZrRa1tXv0t+0n9dHOk2r3+DQ0PkxfnTdKY4cPMvk3AAAAQE+goMEv/H3HSb29oVBfnTtKt00b\nZnacHlHf7NLOvErtOFSu42Udr/UAu1UTRsVqemaCrhsRowD7hU/9rG1s07ufHtPWnHIZksaPjNFX\n5o7S4NiBOyUUAACgP6KgwS+0tLn1499tVViIXb96fIZs1v6xRk1Lm1t78qu041CFDhfVyZBktViU\nmRqtaRkJumFMnEKCun66Z1F5k7LWF+jwyXpZLRbNnpisxTelKiI0sOd+CQAAAPQaChr8xh/W5mvD\n5yV6YvE4TU6PNzvOVWt3e7X/aI2255br4LEaebwdf41GDYnUtIwETUmPv6ZCZRiG9hfW6O0NhSqv\nbVVwoE13zkjRgslDFRjAQiIAAAB9GQUNfqOspkX/vGqHRg+J1D89MsnsOFfE4/Upr6hO23Mr9HlB\nlVztXknSkLhQTctM0LSMBMVGhXT7c27aV6r3PzuuZqdbMRFBunfOSE3LTJDVwkIiAAAAfREFDX7l\n/729XweP1eiZb07W8MQIs+Ncks8wdLSkQdsPVWj34Uo1tbolSbGRwR2lLDNBQ+LCejxHa5tHf912\nQh/vLpbH61NqUrgemDdaY4ZG9fhzAwAAoHtR0OBXco7X6LdZ+zVjbKIeuzvT7DjnMQxDxVUt2n6o\nXDsPVaqmsU2SFOEI0JT0BE0bm6CRyRGymDCCVV3vVPamo9qZVylJumFMnL5y80glDHL0ehYAAABc\nHQoa/IphGPrZ6h2qrHPq10/MVFRYkNmRJElV9c7OZfFLqlskScGBNk0aE6dpYxOUkRLtNwubHC1t\nUNb6QhUWN8hmtWju9YN1z02pCgvp/5cvAAAA6OsoaPA7G/eW6I2P8nXPjcO1eNaIXn9+wzDU0uZR\nfbNLh4vqtONQhY6WNkqS7DaLJoyM1bTMBI0fGeO3i3IYhqE9+VXK3nhUlfVOOYLsumvmcN0yachF\nl/IHAACA+Sho8Dsut1c//t0WWa0WPffETAXYu6cEudq9amhxqaGlXQ3N7R1/trSrscX1pZ/b5fV9\n8dK3WKSMlGhNy0zQpDFxcvShC2l7vD6t31OsD7aeUEubR3FRwbr/5lGanBZnyjRMAAAAXBoFDX4p\ne+NRfbi9SI/eka5Z45Mvejuvz6fGFrcaW9o7ytdZRauhpV2Nza7O79tOr6x4MXabVVFhgYoMDVRE\naKAiw4KUHOPQlPR4RfrJVMur1ex064MtJ7T+82J5fYZGDY7UA/NGaeTgSLOjAQAA4CwUNPil2sY2\n/eNL25QU69CdM1LU+KXi1dDcMfLV1OrWpV6kFknhoR2l68xXRFigIkODFBkaqKiw02UsNEghQbZ+\nP6pUUdeq7A1HtedIlSRpaka87pszUnHdfAkAAAAAXB0KGvzWy+/ndK5I+GUhQTZFnFeyThevsC/K\nWJgjwG8W7/AnR07VK2t9gY6XNclus2j+5KG6a0ZKn5q+CQAA0B9R0OC36ppc2nKwTKHB9o4ydtb0\nwyA/XZyjL/EZhnbmVejPG4+qptGlsJAALbopVXMmJstuo9QCAACYgYIGDHBuj1cf7y7WX7edkNPl\nVeIgh74yd6Qmjort91M+AQAA/A0FDYAkqbG1Xe9/dlyb9pbKZxhKHxalb9yeroTogXWh69LqFv3P\n3/JUXd+mkCC7QoLscgTZOr8PCbLLEXxm+1nbguwKCbLJERygkCAbU2sBAMBVoaABOEdpdYv+tKFQ\n+4/WKDjQpkfvyNCU9HizY/WK7YfK9fu/5cvl9io2Mlgut1etbZ5zLrvQVYEB1ouUuC/K3Nnbz5S+\ns7dZrYxgAgAw0FDQAFzQttxyvfH3jrIy74bBemDe6H57kWu3x6c/flKgDXtLFBRo07fOKqWGYcjt\n8cnp8qjV5ZHT5ZXT5Tnr57O+bzt7m/ec21xpybNInaN1ocEBCg2xyxEcoNAzPwfbFRoSIEeQ/Yvv\nT+8LDuz/K5ICANBfUdAAXFRZTYtefC9HJVUtSkkI1/cWj1V8P5vyWFXv1Evv5ehEeZOGxIXqiSXX\nKXFQ9/6OXy5555W4ti+K3pnbtLR51NrmVkubRy1tbrW7fV1+PqvF0lHWQjqKnCPYrrDgjgLnCA5Q\n2Ok/Q0M6Cp3jrNIXyAI8AACY6poL2vLly7Vnzx55PB4tW7ZMCxcu7Nz39ttvKzs7W1arVenp6Xr2\n2Wcv+akuBQ3wPy63V2+tPaLPDpYpJMimR2/P0OR+MuVxX0G1Vv/lkFpdHt10XZIeXjjGb1cIdXt8\nnYWttc2j5jb3FwXO6Vbr6SJ3Zn/LWfuuZPTObrNqUESQrh8dq6kZCRqeGM5oHAAAveiaCtr27dv1\n6quvatWqVaqrq9OSJUu0ceNGSZLT6dTjjz+u1atXKyAgQEuXLtXTTz+tG2644aKPR0ED/NeWg2X6\nw0f5avf4NH/SEH113qg+uxy/1+fTO5uO6W87TirAbtUjC8Zo1oRks2P1CMMw1O72nVXevihuZ0bn\nzi13bpXXtsrp8kqS4qKCNSU9QVMz4jU0PoyyBgBAD7umgub1euVyueRwOOT1ejVz5kxt3bpVNtu5\nn0A7nU49/PDDWrFihYYOHXrRx6OgAf6tpKpZL76Xo7KaVqUmhet7i8YpNirE7FhXpL7ZpZffz9WR\nU/WKjw7RE4vHaVjCxQ+EA5Hb41PO8RrtyqvU3sJqudo7ylriIIemZsRrSkaCBseGmpwSAID+qdvO\nQcvKytLu3bv161//+pztr7zyit544w0tXbpU3/3udy/5GBQ0wP+52r36w9p8bc0plyPIrm/fmaHr\nx8SZHatL8k7UauWaXDW2ujU5LU6P3pGhkCC72bH8WrvbqwNHa7TzcKUOFFar3dNxLtzguFBNTY/X\n1IwEJXTzOXsAAAxk3VLQ1q1bp5UrV+q1115TePj5D9jW1qbHHntMTz/9tCZNmnTRx6GgAX2DYRj6\n7ECZ3vz4iNwenxZOGar7bx7pt1MefYahv249ofc+Oy6rxaKvzhul+ZOGMF3vCrW1e7S/sEY78yp0\n8FitPN6OsjYsIUxTMxI0NT2+z42oAgDgb665oG3evFkrVqzQ6tWrFRUV1bm9vr5eBQUFmjJliiRp\n1apVkqTHHnvsoo9FQQP6luLKjimP5bWtGpEcoccXjVVspH+9QW92urXqg0M6eKxGgyKC9L1F4zRy\ncKTZsfq81jaP9hZUadfhSuUer+1ciCQ1KaJjGmR6vAZFBJucEgCAvueaClpTU5Meeughvf7664qJ\niTlnX3V1tR544AGtWbNGoaGheuqpp3TPPfdo/vz5F308ChrQ97S1e/TGR/nanluh0GC7vn1XpiaO\nijU7liTpaEmDXno/R7WNLo0bMUiP3ZWpcEeg2bH6nWanW58fqdKuvArlFdXLd/qfjtFDIjU1I0GT\n0+IUGRZkckoAAPqGaypoWVlZeuGFF5Samtq5bdq0aUpLS9OCBQv0zjvv6K233pLdbldaWpr+9V//\nlWX2gX7IMAx9ur9Ub31cII/Xp9umDdO9s0eYNuXRMAyt212stzcUymcYWjxrhO6ckSIrUxp7XGNr\nu/bkd5S1/JP1MiRZLFLa0ChNzUjQpLQ4SjL8jtPl0Z78KjmC7Uoc5FB8dIjfTtkG0P9xoWoA3eZk\nRZNeei9HFXVOjRocqccXje31aW5Ol0f/82GedudXKcIRoGX3jFXG8EG9mgEd6ppc2p1fqV15lSos\naZDUcRHtjOHRmpoerxvS4hQaHGBySgx01fVOrfjzAZVUtXRus1osiosKVlJMqBIHOZQY41DiIIeS\nYhx8wACgx1HQAHQrp8uj3//9sHbmVSosJEDfuStT40fGXP6O3eBUZbNefPegKuqcGjMkUssWjVN0\nOFPr/EFNQ5t2Ha7UrsMVOl7Wcay3WS0alzpIUzMSNHF0LCtqotcdOVWv/37noJqdbs2ekKyE6BCV\n1bSqvLZVZTUtamnznHef0GB7R3GLcSjprPIWF8WoG4DuQUED0O0Mw9DGvSX6v08K5PEaumN6ipbM\nTpXN2nNvXjbvL+1cVfL26R1TLHvy+XD1Kuud2pVXoV15lTpZ2SxJstusGj8yRlMz4jV+ZIyCAylr\n6Fmb95fqjY/yZRjSwwvHaO71g8+7TVNre2dhKz+ruFXVt3Wea3mGzWpRXFSIkk4Xto4C11HkwkIY\nKQbQdRQ0AD2mqLxjymNlfc+NaLncXr219og+O1gmR5Bd37krUxNH+8ciJbi8spoW7cqr1M7DlSqt\n7phiZrNaNHJwpDJTopUxPFqpSRGMTKDb+HyG3t5QqLW7Tik02K4nFo+74mnQHq9PlXXOzsJ2psCV\n1bSq1XX+qFtYSEBncTszbTIpxqHYqGA+SAJwHgoagB7V2ubR63/rOCcsLCRA3707U+NGdM+Ux/La\nVr347kEVV7UoJTFcTywepziuw9VnFVc1a2depXKO1aiovEln/gEKCrQpbWiUMlKilTl8kAbHhbLg\nC65Ka5tHL6/JUc6xWiXFOPTU/eOVEN19F1o3DENNre5zituZEbiqeqe+/K7KZrUoPjpEiYMcSk2K\n0C2ThjDVFwAFDUDPMwxD6z8vUdb6Anm9hu6cOVyLb0qV1Xr1b7J3Ha7U/3yYp7Z2r+beMFhfmzda\nAXY+ie4vWtrcOlxUr0NFtco7Uafy2tbOfeGOAGWkRHcWNko5uqKirlXPZx9QWU2rxo0YpMfvGSdH\ncO+VIbfHp8p65+mpki0dI26nC5zz9KhbuCNAS2aN0KwJSYysAQMYBQ1Arzle1qiX3stRdUOb0odF\n6bv3jFXUFV4fy+P16e31hVq3p1hBATZ94/Y0Tc9M7KHE8Be1jW3KK6rToRN1yiuqVX1ze+e+2Mhg\nZQ6PVkbKIGWkRCsilFX2cK68ojq9+O5BtbR5tHDKUH117qhr+oCoOxmGocaWdn16oEwfbiuSy+3V\n4LhQfW3eaI1NZQVaYCCioAHoVa1tbr324WF9fqRjGfzv3jNWmV08/6OmoU0vvZ+jY6WNSo4N1ROL\nxyk5NrSHE8PfGIah8trW02WtToeL6s4572dIXNjpwhatMUOjmDI2wG3cW6K3Pj4iSfr6rWmaPSHZ\n5EQXV9/s0jufHtOWA2UyJE0YGaOvzhulpBiOc8BAQkED0OvOuZC0z9DdNw7XPTdeesrjgaM1WvVB\nrlraPJoxNlFLb01TUKCtF1PDX/l8hooqmnToRK3yiupUUNwgt8cnqeMcn9SkiM7CNnJwJAuODBBe\nn09/XFeoTz4vVlhIgL6/ZJzShkWbHatLisqb9MdPCpR/ql42q0Vzrx+se25KZTVIYICgoAEwzbHS\njimPNY1tykiJ1nfvzlTkl6ZAN1/SAAAZP0lEQVQ8+nyG3vvsmP6ytUh2m1UPLxit2ROSZWGRCFyE\n2+NVYXGDDhV1jLAdL2vsXJwhMMCqMUOilDm8Yzrk0IQwFhzph1ra3HrpvRwdOlGnwXGheuq+8X3u\nXEXDMPT5kWr9aUOhKuudCg22656bUjX3+sF8yAD0cxQ0AKZqdrr12l/ztK+wWpGhgfruPWOVkdLx\nKXdDs0sr1+Tq8Ml6xUUF64nF1ykl8eIHLeBCWtvcyj9Z31nYziznL3Usf54+LEoZwwcpc3i04qNC\nKP99XFlNi57PPqCKOqcmjorVY3dn9ulprm6PT5/sKdYHW0/I6fIocZBDX503ShNGxvBaBfopChoA\n0xmGobW7Til741H5DEOLbkrV6CFRemVNrhpa2nX96Fh9+84MOYKZ3oNrV9fk0uGiuo4VIovqVNvo\n6twXExGk26alaN4Ng3nz2wflHq/VS+/lqNXl0e3Th+m+2SP9ZjGQa9XY2q73Nx/Xxn0lMgxp7PBo\nPXDLaA2JCzM7GoBuRkED4DcKSxr08vs5nW+YrRaLvjJ3pBZOGcqbZfQIwzBUWefUoRO1OlRUp0Mn\nauV0eTVxVKwevSNd4Q5WhOwLDMPQJ3uK9cdPCmW1St+8PV0zxyWZHatHlFQ1K2t9oXKO18pikeZM\nSNbiWSNYvRToRyhoAPxKs9Ot1/92WMWVzfr2XRkaPSTK7EgYQOqaXFr9l0PKK6pTZFigHrsrs8ur\njMIcHq9P//vxEW3cV6oIR4CevG+8Rg2ONDtWjztwtEZZ6wtUVtOqkCCb7poxXPMnD+V6kEA/QEED\nAOAsPsPQ33ec1LufHpPPZ+i26cO0ZNYIFmbwQ81Ot15896AOn6zXsPgw/eC+8YqJDDY7Vq/xeH3a\ntK9U720+ppY2j+KigvWVm0dpUlocsw6APoyCBgDABRwrbdQra3JVWe9UalK4lt0zVvHRDrNj4bSS\n6hY9n71fVfVtumFMnB67K3PAXnqjpc2tD7ac0Cd7iuX1GRozNEpfu2WUhidGmB0NwFWgoAEAcBFO\nl0dvrj2ibbnlCgq06esLx/Tbc5v6kgNHq/Xy+7lqa/fq7pnDtWhWKpdLkFRR26q3NxRqb0G1LJJm\njkvUvXNGKjo86LL3BeA/KGgAAFzGttxy/eGjfLW1ezV9bIK+vjCtTy/d3ledWfH17Q2Fstus+tYd\nGZqWmWB2LL+Td6JW//dJoYqrmhUYYNUd01N069RhCgoYmCOMQF9DQQMAoAsq61q1cs0hHS9rVFxU\nsL57z1iNTO7/i1H4C7fHpz98lK/PDpYpMixQT903XqlJTOG7GJ/P0GcHy/TOp8fU2NKu6PAg3X/z\nSE3LTGC0EfBzFDQAALrI4/Xp/c+O68NtRbJaLVo8K1W3T0vpN9fa8leNLe363bsHVVDcoJTEcD11\n33im7XWR0+XRh9uL9NHOU/J4fRqRHKGv3TJ6QKx0CfRVFDQAAK5Q3olarfrLIdU3tyt9WJQeu3ss\nhaGHnKps1vPZB1TT2KapGfF69I4Mpupdhep6p/608ah2Ha6UJE3NiNf9N49UbGSIyckAfBkFDQCA\nq9DU2q7/+fCw9hVWKywkQI/eka7rR8eZHeuqudq9amztmArnL5cU2HukSq98cEgut1eLZ6Xq7pnD\nWT7+GhUU1+uPnxToeFmTAuxWLZwyVHdMT+GcSsCPUNAAALhKhmFow94S/fGTQnm8Ps27YbC+OneU\nAvvICI9hGCosadDm/WXadbhSLrdXFos0KDxIMZEhiosMVkxksOKiQhQbGazYyBBFhwf1+JROwzD0\n4fYivbPpmALsVn3nrkxNTo/v0eccSHyGoR25FcredFR1TS5FhgbqzhkpmjkuUY7gALPjAQMeBQ0A\ngGtUXNmslWtyVVLdosFxoVp2z1gNiQszO9ZFNba0a2tOuTYfKFVZTaskKTYyWCMHR6qusU1VDW2q\nb3LpQm8CbFaLBkUEKTbydGk7Xd7iIkMUExmsyLDAa1qEwu3x6vW/Hda23ApFhwfpqfvGKyXx4m9W\ncPVcbq8+2nFSH+4oUrvbp0C7VVMy4jVn4mCNTI5gtBIwCQUNAIBu0O72KmtDoTZ8XqIAu1UPzBul\nudcP9ps3uT6foZzjNdq8v0z7Cqvl9Rmy2yy6YUycZk9IVnpK9DnFyu3xqbapTdX1bapucKq6oa3j\nq77j+4aW9gs+j91mPT3a9kV5OzP6FhsVrPCQgIv+N2lodumFdw7qWGmjRiRH6Af3XqfIMM7t62kN\nLe3acrBMn+4rVWW9U5I0OC5Usycka+a4RIUyqgb0KgoaAADdaO+RKr32YZ5a2jy6fnSsHr0jQ2Eh\n5r3Brax36rMDpdpysFx1TS5J0pC4MM2ekKTpYxOvOlu726uaxjZV1beppsGpqi8VuGan+4L3Cwqw\nnVfaYiODFWC36vd/z1ddk0szxibom7enK8DeN6aK9hc+w1B+UZ027S/VnvwqeX2GAuxWTU6L15yJ\nyRo9JNJvPnAA+jMKGgAA3ayuyaVVH+Tq8Ml6RYUF6rG7xyojJbrXnt/t8WpPfpU2HyhTXlGdJCkk\nyKZpmYmaNT5JwxPDe/yNttPlUU1jxwhcVYNTNQ1tqqo//WdDm5wuz3n3sUi6d84I3TE9hSJgssbW\ndm09WK5N+0pUUdcxqpYU49CcCcmaeV2SqR86AP0dBQ0AgB7g8xn6244ivfvpcRmGoTtmpGjRTak9\nukLiyYombd5fpm255Wo9XYDGDI3SrPFJmpwe71fL07e2uVVVf3rUrcGpuiaXrhsRo7Gpg8yOhrMY\nhqH8k/X6dH+pdudXyuPtmBp7ZlRtzNAoyjTQzShoAAD0oKMlDVq5JlfVDW0akRyh794zVvFR3Xft\nqdY2t7YfqtDm/WUqquj4dzQyNFA3XpekWeOTlDDI0W3PhYGtqbVd23LKtWn/F4vLJAw6M6qWqAhH\noMkJu8bnM1Ra06LjpY06VtYol9urJbNGKK4b/14C14KCBgBAD2tt8+jNtfnafqhCwYE2ff3WNM0Y\nm3jVj3dmVGPzgVLtzq+S2+OT1WLR+JExmjUhSeNHxshm9Y9rmaH/MQxDBcUN2rSvRLsOV8nj9clm\ntWhS2oUXnDFbXZNLx0obdbysUcdKG3SivElt7d5zbhMWEqAf3HedRg+JMikl8AUKGgAAvcAwDG3N\nKdebHx+Rq92rmeMS9fCCMVd0geC6Jpe25pRp8/6yztX24qNDNGt8km68LklRrHiIXtbsdGtbbrk+\n3VeqkuoWSVJ8VIhmT0zWjdclKTK0d0fV2to9Kipv0rHTo2PHShs7F8c5IynGoRHJERqRFKERyZE6\nVtqgtz4ukNUqPXp7hmaMu/oPT4DuQEEDAKAXVdS1auX7uTpR3qT46BAtu2esUpMiLnp7j9enA0dr\ntHl/qQ4cq5FhSIF2qyalxWv2hCTOAYJfMAxDR0satWlfiXYerpTb0zGqNnF0rOZMTFbm8EHdPqrm\n8xkqrW45XcQadKy0SSXVzTr73WtEaKBGJEUoNTlCI5IjlJoYIUfw+R+K5J6o1Yvv5sjp8uiumSla\nPGuEX40CYmChoAEA0Ms8Xp/e/fSY/rbjpGxWi+6dPUK3Tht2zhvC8tpWbd5fqi055Wo8fc2x4Ynh\nmjUhWdMyEi74JhPwB61tbm3LrdCmfSUqruoYVYuNDNbsCcm6afzVj/R2TFVs6BgdK23UifImudxf\nTFUMtFuVkhjeUcSSOgpZTERwlz/AKKtp0Yo/HVBlvVOT0+L07bsy/WphHQwcFDQAAEySe6JWqz84\npIaWdmWkRGvpbWkqLG7Q5v2lOlLcIEkKDbZr+tiO5fGHJVz8H23A3xiGoWNljdq0r1Q78yrU7u44\nV3LCqBjNmThY41IHyWq9cHlqa/foRFlT5zTF42XnTlW0SEqKDT09TbGjkA2OC73mVVKbnW797p2D\nyj9Vr+GJ4frBfeMVHc7UYfQuChoAACZqbG3Xa3/N04GjNedsz0iJ1qwJSZo0Jo4LNqPPc7o82n6o\nQpv2luhkZbMkKSYiSLMmJOvGcUlqdXm+GB0ra1Rpdcs5UxUjQwM7zhs7XcaGX2SqYnfweH1646N8\nfXagTNHhQXrqvvFKSeTDEfSeay5oy5cv1549e+TxeLRs2TItXLiwc9/27dv129/+VlarVampqfrP\n//xPWS+xqhQFDQAwEBmGofWfl2hHXoXSh0XrpvFJ3boUP+AvDMPQifImbdpXqh2HKs6ZonhGoN2q\n4YnhGpEc2XHuWFKEBkUE9eq5loZh6KOdp/SnDYUKCLDqsbvGalJaXK89Pwa2aypo27dv16uvvqpV\nq1aprq5OS5Ys0caNGzv3L1y4UG+88YYSExP11FNP6b777tOcOXMu+ngUNAAAgIHB6fJoZ16F9hyp\nUlRYUOd0xcFxoX5zmYi9BVV6Zc0hudxe3X/zSN0+bRiL8qDHXVNB83q9crlccjgc8nq9mjlzprZu\n3SqbrWMqRnNzs8LCwiRJP//5zzVx4kQtXrz4oo9HQQMAAIA/OVnRpBXZB1TX5NKN4xK19LZ0Bdj9\no0Cif7pUQbvsK89ms8nhcEiSsrOzNXv27M5yJqmznFVWVmrLli2XHD0DAAAA/M2whHD9yzcmKzUp\nXFtyyvWbP+5VU2u72bEwQHX5o4F169YpOztbzzzzzHn7ampq9Pjjj+vZZ59VdHR0twYEAAAAelpU\nWJD+v4du0JT0eB0pbtB/vLFbpacvzA30pi4tErJ582atWLFCq1evVlRU1Dn7mpubtXTpUj399NOa\nPXv2ZZ+QKY4AAADwVz7D0JrPjmvNlhMKCbLre4vHalxqjNmx0M9c0zloTU1Neuihh/T6668rJub8\nF+fPfvYzTZkyRYsWLepSGAoaAAAA/N223HL9z4eH5fMZenD+aN0yaYjZkdCPXFNBy8rK0gsvvKDU\n1NTObdOmTVNaWppuuukmTZkyRddff33nvrvuuksPPPDARR+PggYAAIC+oLCkQf/95wNqbHXrlhuG\n6GvzR/nN6pPo27hQNQAAAHAVquudWvHnAyqpatG41EF6fNG4HruANgYOChoAAABwlZwuj1auydWB\nozVKjg3VU/eP50LzuCYUNAAAAOAa+HyG3t5QqLW7TiksJEBP3nudxgyNuvwdgQugoAEAAADdYOO+\nEr219ogsFukbt6XrxuuSzI6ESzAMQxaLxewY56GgAQAAAN3k0IlavfhujlpdHt05I0VLZo+Q1Q9L\nwEB2vKxR//vxETnbvfqP70wzO855LlXQWIYGAAAAuAKZwwfpn5dOUnx0iP66rUgvvZsjV7vX7FiQ\n1Ox0642/H9Z//H63jpY2aszQKPXyeNQ1YwQNAAAAuArNTrdefPegDp+sV0pCuJ66f7yiw4PMjjUg\n+QxDnx0oU/bGo2p2upUcG6pHFoxRekq02dEuiCmOAAAAQA/weH36w0f52nygTFFhgXrq/vEanhhh\ndqwB5UR5o95ce0THShsVFGjTohtTNX/yENlt/jtZkIIGAAAA9BDDMPTRzlP604ZCBditeuzuTE1K\nizc7Vr/X7HTr3U+PaePeEhmSpmbE64F5o/vEKCYFDQAAAOhhewuq9MqaQ3K5vbpvzgjdMT3FL1cQ\n7Ot8hqEtB8r0p9PTGZNiHHpkwRhlDB9kdrQuo6ABAAAAveBkRZOe//MB1Ta6NHNcor5xW7oC7P47\n1a6vKSpv0psf5+toSaOCAmy658bhWjBlqF9PZ7wQChoAAADQSxqaXXr+zwd1vKxRo4dEatk9YxUW\nEiC7zSqrlRG1q9Ha5tY7nx7Thr0lMgxpcnq8vjZvlAZFBJsd7apQ0AAAAIBe1O726tW/5mnX4cpz\ntlsskt1mPf1lkd1mlc1q6fzZdma79eyfT39vPes+Nss5j2G3WWW3nnX/07cJsFmVkhiu2MgQk/5L\nXBufYWhbTrne3lCopla3EgZ1TGccm9p3pjNeCAUNAAAA6GWGYejj3cU6XFQnj88nj8cnj8+Q1+uT\nx2vI4/XJ6zU69nm/tN3XvW/RU5PCNSktXpPS4pQQ7ejWx+4pJyua9ObHR1RY3KDAAKvunjlcC6cM\n6xdTRiloAAAAQB9iGIa8vo6ydk55O6vMuc8UvLNv03mfjn1t7V7lnqhV3ok6+U6/7R8WH6ZJaXGa\nnB6vpJhQk3/T87W2efTe5mP65PNiGYY0KS1OX5s3WjGRfXM644VQ0AAAAIABrNnp1t6CKu3Jr1Lu\n8drOEbrk2FBNTovT5LR4DY4LNXXVScMwtC23XG9vOKrGlnYlRIfo4QVjNG5EjGmZegoFDQAAAICk\njhGq/YXV2p1fqYPHauXx+iRJCdEhmpwer8lp8RqWENarZa24sllvrs3XkeIGBdqtumvmcN06tX9M\nZ7wQChoAAACA8zhdHh08VqPd+VU6cLRa7e6OshYbGazJafGalB6nEUkRPVbWnC6P3v/suNbtLpbP\nMHT96Fg9eMtoxUb1zUVNuoqCBgAAAOCSXG6vco7Vak9+pfYVVqut3StJig4P6jhnLS1eo4ZEytoN\nZc0wDO04VKGs9YVqaGlXfFSIHlowWuNHxl7zY/cFFDQAAAAAXeb2eJV7ok57Dldqb0G1Wl0eSVJk\naKBuOF3WxgyNlM165VMQS6qa9ebaI8o/Va8Au1V3zkjR7dOGKcBu6+5fw29R0AAAAABcFY/Xp7yi\nOu3Jr9TnR6rV7HRLksJCAnTDmDhNTotTekq07LZLlzWny6M1WzqmM3p9hiaOitWD80crrp9PZ7wQ\nChoAAACAa+b1+XTkZL1251dpz5EqNba0S5JCg+2aODpWk9LiNXb4oHMW9zAMQzvzKpW1vkD1ze2K\njQzWQwvGaOKogTGd8UIoaAAAAAC6lc9nqLCkQbsPV2rPkSrVNbkkSSFBNk0Y2VHWYiOD9faGQuUV\n1clus+qO6cN0x/QUBQYMnOmMF0JBAwAAANBjfIah46WN2p1fqd2Hq1TT2HbO/vEjY/TQ/NGKj3aY\nlNC/UNAAAAAA9ArDMFRU0aTdh6tUXNWsOROTNXFUrKkXwfY3FDQAAAAA8BOXKmj989LcAAAAANAH\nUdAAAAAAwE9Q0AAAAADAT1DQAAAAAMBPUNAAAAAAwE9Q0AAAAADAT1DQAAAAAMBPUNAAAAAAwE/0\n+oWqAQAAAAAXxggaAAAAAPgJChoAAAAA+AkKGgAAAAD4CQoaAAAAAPgJChoAAAAA+AkKGgAAAAD4\nCQoaAAAAAPgJu9kB/MEvfvEL7d+/XxaLRT/96U81fvx4syOhn9uxY4f+4R/+QaNHj5YkjRkzRv/y\nL/9icir0Z0eOHNETTzyhb37zm3rkkUdUVlamf/zHf5TX61VcXJx+/etfKzAw0OyY6Ie+/Nr7yU9+\notzcXEVFRUmSvv3tb+vmm282NyT6neXLl2vPnj3yeDxatmyZrrvuOo556BVffu2tX7/+io95A76g\n7dy5U0VFRcrKytLRo0f105/+VFlZWWbHwgAwdepUPf/882bHwADQ2tqqf//3f9eMGTM6tz3//PN6\n6KGHdPvtt+u3v/2tsrOz9dBDD5mYEv3RhV57kvSjH/1Ic+fONSkV+rvt27eroKBAWVlZqqur05Il\nSzRjxgyOeehxF3rtTZ8+/YqPeQN+iuO2bds0f/58SdLIkSPV0NCg5uZmk1MBQPcJDAzUqlWrFB8f\n37ltx44duuWWWyRJc+fO1bZt28yKh37sQq89oKdNmTJFK1askCRFRETI6XRyzEOvuNBrz+v1XvHj\nDPiCVl1drejo6M6fBw0apKqqKhMTYaAoLCzU448/rgcffFBbtmwxOw76MbvdruDg4HO2OZ3Ozuk9\nMTExHPfQIy702pOkN998U0uXLtUPf/hD1dbWmpAM/ZnNZpPD4ZAkZWdna/bs2Rzz0Csu9Nqz2WxX\nfMwb8FMcv8wwDLMjYAAYPny4nnzySd1+++06deqUli5dqrVr1zIfHqbguIfetGjRIkVFRSkjI0Ov\nvPKK/vu//1vPPPOM2bHQD61bt07Z2dl67bXXtHDhws7tHPPQ085+7eXk5FzxMW/Aj6DFx8erurq6\n8+fKykrFxcWZmAgDQUJCgu644w5ZLBYNGzZMsbGxqqioMDsWBhCHw6G2tjZJUkVFBVPQ0GtmzJih\njIwMSdK8efN05MgRkxOhP9q8ebNefvllrVq1SuHh4Rzz0Gu+/Nq7mmPegC9oN954oz766CNJUm5u\nruLj4xUWFmZyKvR3a9as0auvvipJqqqqUk1NjRISEkxOhYFk5syZnce+tWvXatasWSYnwkDxgx/8\nQKdOnZLUcS7kmdVsge7S1NSk5cuXa+XKlZ0r53HMQ2+40Gvvao55FoNxXj333HPavXu3LBaLnn32\nWaWnp5sdCf1cc3OzfvzjH6uxsVFut1tPPvmk5syZY3Ys9FM5OTn6r//6L5WUlMhutyshIUHPPfec\nfvKTn8jlcik5OVm//OUvFRAQYHZU9DMXeu098sgjeuWVVxQSEiKHw6Ff/vKXiomJMTsq+pGsrCy9\n8MILSk1N7dz2q1/9Sj/72c845qFHXei1d++99+rNN9+8omMeBQ0AAAAA/MSAn+IIAAAAAP6CggYA\nAAAAfoKCBgAAAAB+goIGAAAAAH6CggYAAAAAfoKCBgAAAAB+goIGAAAAAH7i/wd72ymk2Ov2YwAA\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1080x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "HExGAmdDalm0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Introduction\n",
        "The experiment task consisted of building a neural network to perform multi-class classification on a supplied dataset without the use of Deep Learning frameworks (e.g. TensorFlow, Caffe, and KERAS). The dataset consisted of 60,000 labeled training samples and 10,000 unlabeled test samples. The structure of the data (e.g. image, video, etc) was unknown. The performance of the neural network was evaluated in terms of the accuracy metric. Various neural network structures and parameters were trailed to maximise speed and accuracy.\n",
        "\n",
        "The objective of building the neural network without Deep Learning frameworks was to gain a comprehensive understanding of the math and mechanics behind neural networks.\n",
        "\n",
        "\n",
        "\n",
        "##SGD with Momentum\n",
        "Momentum ($v_t$) is an exponentially weighted average of a neural networks gradients. It is used to update the weights ($w_t$) and baises ($b_t$) of a network.\n",
        "\n",
        "$$v_t = \\beta v_{t-1} + \\eta \\nabla_w J(w)$$\n",
        "$$w_t = w_{t-1} - v_t$$\n",
        "\n",
        "Momentum increases for features whose gradients point in the same direction and reduces for features whose gradients change direction. By reducing the fluctuation of gradients convergence is generally sped up. The hyper-parameter $\\beta$ takes a value between 0 - 1 and dictates how many samples are included in the exponential weighted average. A small $\\beta$ value will increase fluctuation because the average is taken over a smaller number of examples. A large $\\beta$ will increase smoothing because the average is taken over a larger number of examples. A $\\beta$ value of 0.9 provides a balance between the two extremes.\n",
        "\n",
        "##Gradient Descent\n",
        "Gradient descent is a machine learning optimization method. In deep learning it is used to calculate the model parameters (weights and biases) that minimise the cost function. The gradient descent method invovles iterating through a training dataset and updating weights and baises in accordance with the gradient of error. There are three types of gradient descent. Each uses a different number of training examples to update the model parameters:\n",
        "*   **Batch Gradient Descent** uses the entire training dataset to calculate gradients and update the parameters. Because the entire training dataset is considered parameters updates are smooth however, it can take a long time to make a single update.\n",
        "*   **Stochastic Gradient Descent (SGD)** uses a single randomly selected sample from the training dataset to calculate gradients and update the parameters. Parameter updates are fast but very noisey.\n",
        "*   **Mini-batch Gradient Descent** uses a subset of the training data (e.g. batches of 1000 samples) to calculate gradients and update the parameters. Mini-batch gradient descent is a compromise between batch and stochastic gradient descent. The mini-batch size can be adjusted to find the appropriate balance between fast convergence and noisey updates. "
      ]
    },
    {
      "metadata": {
        "id": "BRDGNA5IPpGY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Louis testing stuff below"
      ]
    },
    {
      "metadata": {
        "id": "WLXWBGImOPVM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.seterr(all=\"warn\")\n",
        "np.random.seed(1)\n",
        "procdata = np.copy(data)\n",
        "preprocess(procdata, 'zscore')\n",
        "\n",
        "#split data\n",
        "train, train_target, validate, validate_target = split(data, label)\n",
        "#one hot encode targets\n",
        "train_target = OHE(train_target, 10)\n",
        "validate_target = OHE(validate_target, 10)\n",
        "second_layer = False\n",
        "relu = False\n",
        "\n",
        "nn = MLP([128,60,10], [None, 'tanh', 'tanh'], False)\n",
        "start = time.time()\n",
        "MSE = nn.fit(train, train_target, learning_rate=0.001, epochs=5)\n",
        "# MSE = nn.fit_mb(train, train_target, 500, learning_rate=0.001, epochs=10)\n",
        "print(\"{}s to train\".format(time.time() - start))\n",
        "print('loss:%f'%MSE[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dv2bA_5yPoPg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "np.seterr(all=\"warn\")\n",
        "np.random.seed(1)\n",
        "procdata = np.copy(data)\n",
        "preprocess(procdata, 'zscore')\n",
        "\n",
        "#split data\n",
        "train, train_target, validate, validate_target = split(data, label)\n",
        "#one hot encode targets\n",
        "train_target = OHE(train_target, 10)\n",
        "validate_target = OHE(validate_target, 10)\n",
        "second_layer = False\n",
        "relu = False\n",
        "if second_layer:\n",
        "  nn = MLP([128,60,30,10], [None,'logistic','logistic','tanh'])\n",
        "elif relu:\n",
        "  nn = MLP([128,60,10], [None, 'relu', 'relu'], False)\n",
        "  start = time.time()\n",
        "  MSE = nn.fit(train, train_target, learning_rate=0.001, epochs=500)\n",
        "  print(\"{}s to train\".format(time.time() - start))\n",
        "else:\n",
        "  nn = MLP([128,60,10], [None,'logistic','tanh'], init_uniform=False, weight_decay=0.1)\n",
        "  start = time.time()\n",
        "  MSE = nn.fit_mb(train, train_target,20, learning_rate=0.01, epochs=5)\n",
        "  print(\"{}s to train\".format(time.time() - start))\n",
        "print('loss:%f'%MSE[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3fVa6fliErm5",
        "colab_type": "code",
        "outputId": "5f8091e2-23fc-41cf-973c-8b072e42e9a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "cell_type": "code",
      "source": [
        "pl.figure(figsize=(15,4))\n",
        "pl.plot(MSE)\n",
        "pl.grid()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3MAAAD4CAYAAABG1r/8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAE/RJREFUeJzt3E+MVeX9x/HPBRmtMiCTzCgWFoRo\nTIyKBGLCJMACNKXGxOIINOxorCmL1kzaIl3AQqkQNTWIxT8YiSEwgGiNC22MkJgy1VqSwdBFhYUO\nMcK9Uf4Mavjj/S2ME+cnFb1OZ3w6r9fu3OeeO9+bPAvenHNupV6v1wMAAEBRRg33AAAAAHx3Yg4A\nAKBAYg4AAKBAYg4AAKBAYg4AAKBAFw33ABdSrZ4c7hEAAACGTWtr83lfd2UOAACgQGIOAACgQGIO\nAACgQGIOAACgQGIOAACgQGIOAACgQGIOAACgQGIOAACgQGIOAACgQGIOAACgQGIOAACgQGIOAACg\nQGIOAACgQGIOAACgQGIOAACgQGIOAACgQGIOAACgQGIOAACgQGIOAACgQGIOAACgQGIOAACgQGIO\nAACgQGIOAACgQGIOAACgQGIOAACgQGIOAACgQA3H3Jo1a7Jo0aIsXrw4+/fvH7C2d+/e3HnnnVm0\naFE2bNgwYO2zzz7LvHnzsmvXrkb/NAAAwIjXUMy99dZbee+999LV1ZUHHnggDzzwwID1+++/P+vX\nr8/WrVvzt7/9LQcPHuxf+/Of/5zx48d/v6kBAABGuIZirru7O/PmzUuSTJ06NcePH09fX1+SpLe3\nN+PHj8/EiRMzatSozJkzJ93d3UmSQ4cO5eDBg5k7d+7gTA8AADBCNRRztVotEyZM6D9uaWlJtVpN\nklSr1bS0tJx3be3atVmxYsX3mRcAAIAM0g+g1Ov1C77nxRdfzLRp0zJ58uTB+JMAAAAj2kWNnNTW\n1pZardZ/fPTo0bS2tp537ciRI2lra8uePXvS29ubPXv25MMPP0xTU1OuvPLKzJo163t+BQAAgJGn\noZhrb2/P+vXrs3jx4hw4cCBtbW0ZO3ZskmTSpEnp6+vL4cOHc+WVV2b37t156KGHsnTp0v7z169f\nnx//+MdCDgAAoEENxdz06dNz3XXXZfHixalUKlm1alV27dqV5ubmzJ8/P6tXr05nZ2eSZMGCBZky\nZcqgDg0AADDSVerf5oG3YVStnhzuEQAAAIZNa2vzeV8flB9AAQAAYGiJOQAAgAKJOQAAgAKJOQAA\ngAKJOQAAgAKJOQAAgAKJOQAAgAKJOQAAgAKJOQAAgAKJOQAAgAKJOQAAgAKJOQAAgAKJOQAAgAKJ\nOQAAgAKJOQAAgAKJOQAAgAKJOQAAgAKJOQAAgAKJOQAAgAKJOQAAgAKJOQAAgAKJOQAAgAKJOQAA\ngAKJOQAAgAKJOQAAgAKJOQAAgAKJOQAAgAKJOQAAgAKJOQAAgAKJOQAAgAJd1OiJa9asSU9PTyqV\nSlauXJkbbrihf23v3r155JFHMnr06MyePTvLly9Pkqxbty7//Oc/c/bs2fzyl7/MLbfc8v2/AQAA\nwAjUUMy99dZbee+999LV1ZVDhw5l5cqV6erq6l+///77s2nTplxxxRVZunRpbr311tRqtbz77rvp\n6urKxx9/nDvuuEPMAQAANKihmOvu7s68efOSJFOnTs3x48fT19eXsWPHpre3N+PHj8/EiROTJHPm\nzEl3d3d+/vOf91+9GzduXD799NOcO3cuo0ePHqSvAgAAMHI09MxcrVbLhAkT+o9bWlpSrVaTJNVq\nNS0tLV9bGz16dC699NIkyc6dOzN79mwhBwAA0KCGn5n7qnq9/q3f+9prr2Xnzp155plnBuNPAwAA\njEgNxVxbW1tqtVr/8dGjR9Pa2nretSNHjqStrS1J8sYbb2Tjxo15+umn09zc/H3mBgAAGNEaus2y\nvb09r776apLkwIEDaWtry9ixY5MkkyZNSl9fXw4fPpyzZ89m9+7daW9vz8mTJ7Nu3bo88cQTufzy\nywfvGwAAAIxAlfp3uUfyKx566KG8/fbbqVQqWbVqVf71r3+lubk58+fPzz/+8Y889NBDSZJbbrkl\ny5YtS1dXV9avX58pU6b0f8batWtz1VVXfePfqVZPNjIeAADA/4TW1vPf1dhwzA0VMQcAAIxk/ynm\nGrrNEgAAgOEl5gAAAAok5gAAAAok5gAAAAok5gAAAAok5gAAAAok5gAAAAok5gAAAAok5gAAAAok\n5gAAAAok5gAAAAok5gAAAAok5gAAAAok5gAAAAok5gAAAAok5gAAAAok5gAAAAok5gAAAAok5gAA\nAAok5gAAAAok5gAAAAok5gAAAAok5gAAAAok5gAAAAok5gAAAAok5gAAAAok5gAAAAok5gAAAAok\n5gAAAAok5gAAAAok5gAAAAp0UaMnrlmzJj09PalUKlm5cmVuuOGG/rW9e/fmkUceyejRozN79uws\nX778gucAAADw7TUUc2+99Vbee++9dHV15dChQ1m5cmW6urr61++///5s2rQpV1xxRZYuXZpbb701\nH3300TeeAwAAwLfXUMx1d3dn3rx5SZKpU6fm+PHj6evry9ixY9Pb25vx48dn4sSJSZI5c+aku7s7\nH3300X88BwAAgO+moWfmarVaJkyY0H/c0tKSarWaJKlWq2lpafna2jedAwAAwHczKD+AUq/Xh+Qc\nAAAAvtDQbZZtbW2p1Wr9x0ePHk1ra+t5144cOZK2traMGTPmP54DAADAd9PQlbn29va8+uqrSZID\nBw6kra2t/9m3SZMmpa+vL4cPH87Zs2eze/futLe3f+M5AAAAfDcNXZmbPn16rrvuuixevDiVSiWr\nVq3Krl270tzcnPnz52f16tXp7OxMkixYsCBTpkzJlClTvnYOAAAAjanUf+APr1WrJ4d7BAAAgGHT\n2tp83tcH5QdQAAAAGFpiDgAAoEBiDgAAoEBiDgAAoEBiDgAAoEBiDgAAoEBiDgAAoEBiDgAAoEBi\nDgAAoEBiDgAAoEBiDgAAoEBiDgAAoEBiDgAAoEBiDgAAoEBiDgAAoEBiDgAAoEBiDgAAoEBiDgAA\noEBiDgAAoEBiDgAAoEBiDgAAoEBiDgAAoEBiDgAAoEBiDgAAoEBiDgAAoEBiDgAAoEBiDgAAoEBi\nDgAAoEBiDgAAoEBiDgAAoEAXNXLSmTNnsmLFinzwwQcZPXp0/vjHP2by5MkD3vPSSy9l8+bNGTVq\nVO666650dHTk7Nmz+cMf/pD3338/586dy+9+97vMmDFjUL4IAADASNLQlbmXX34548aNy9atW3PP\nPffk4YcfHrD+ySefZMOGDXn22Wfz3HPPZfPmzTl27Fj+8pe/5Ec/+lG2bt2aBx54IA8++OCgfAkA\nAICRpqGY6+7uzvz585Mks2bNyr59+was9/T05Prrr09zc3MuueSSTJ8+Pfv27cvtt9+e++67L0nS\n0tKSY8eOfc/xAQAARqaGbrOs1WppaWlJkowaNSqVSiWnT59OU1PT19aTL8KtWq1mzJgx/a9t3rw5\nt9122/eZHQAAYMS6YMzt2LEjO3bsGPBaT0/PgON6vf6Nn/H/17ds2ZIDBw5k48aN33ZOAAAAvuKC\nMdfR0ZGOjo4Br61YsSLVajXXXnttzpw5k3q93n9VLkna2tpSq9X6j48ePZpp06Yl+SIOX3/99Tz+\n+OMDrtQBAADw7TX0zFx7e3teeeWVJMnu3btz8803D1i/8cYb88477+TEiRM5depU9u3blxkzZqS3\ntzfbtm3LY489losvvvj7Tw8AADBCNfTM3IIFC7J3794sWbIkTU1N/b9K+eSTT2bmzJm56aab0tnZ\nmWXLlqVSqWT58uVpbm7OU089lWPHjuXuu+/u/6xNmzYNuKoHAADAhVXqF3rgbZhVqyeHewQAAIBh\n09rafN7XG7rNEgAAgOEl5gAAAAok5gAAAAok5gAAAAok5gAAAAok5gAAAAok5gAAAAok5gAAAAok\n5gAAAAok5gAAAAok5gAAAAok5gAAAAok5gAAAAok5gAAAAok5gAAAAok5gAAAAok5gAAAAok5gAA\nAAok5gAAAAok5gAAAAok5gAAAAok5gAAAAok5gAAAAok5gAAAAok5gAAAAok5gAAAAok5gAAAAok\n5gAAAAok5gAAAAok5gAAAAok5gAAAArUUMydOXMmnZ2dWbJkSZYuXZre3t6vveell17KwoUL09HR\nkR07dgxYq9VqmTlzZt58883GpgYAABjhGoq5l19+OePGjcvWrVtzzz335OGHHx6w/sknn2TDhg15\n9tln89xzz2Xz5s05duxY//q6desyefLk7zc5AADACNZQzHV3d2f+/PlJklmzZmXfvn0D1nt6enL9\n9denubk5l1xySaZPn97/nu7u7lx22WW55pprvufoAAAAI1dDMVer1dLS0vLFB4walUqlktOnT593\nPUlaWlpSrVZz+vTpbNiwIffee+/3HBsAAGBku+hCb9ixY8fXnnnr6ekZcFyv17/xM75cf/LJJ9PR\n0ZFx48Z91zkBAAD4igvGXEdHRzo6Oga8tmLFilSr1Vx77bU5c+ZM6vV6mpqa+tfb2tpSq9X6j48e\nPZpp06blhRdeyOeff54tW7bk/fffz/79+/Poo4/m6quvHsSvBAAA8L+vodss29vb88orryRJdu/e\nnZtvvnnA+o033ph33nknJ06cyKlTp7Jv377MmDEj27Zty/bt27N9+/bMnTs3q1atEnIAAAANuOCV\nufNZsGBB9u7dmyVLlqSpqSkPPvhgki9uo5w5c2ZuuummdHZ2ZtmyZalUKlm+fHmam5sHdXAAAICR\nrFK/0ANvw6xaPTncIwAAAAyb1tbzXxhr6DZLAAAAhpeYAwAAKJCYAwAAKJCYAwAAKJCYAwAAKJCY\nAwAAKJCYAwAAKJCYAwAAKJCYAwAAKJCYAwAAKJCYAwAAKJCYAwAAKJCYAwAAKJCYAwAAKJCYAwAA\nKJCYAwAAKJCYAwAAKJCYAwAAKJCYAwAAKJCYAwAAKJCYAwAAKJCYAwAAKJCYAwAAKJCYAwAAKFCl\nXq/Xh3sIAAAAvhtX5gAAAAok5gAAAAok5gAAAAok5gAAAAok5gAAAAok5gAAAAok5gAAAAok5hg0\nZ86cSWdnZ5YsWZKlS5emt7f3a+956aWXsnDhwnR0dGTHjh0D1mq1WmbOnJk333xzqEamII3ur7Nn\nz+b3v/99lixZkrvuuitvv/32UI/OD9yaNWuyaNGiLF68OPv37x+wtnfv3tx5551ZtGhRNmzY8K3O\nga9qZH+tW7cuixYtysKFC/PXv/51qEemII3sryT57LPPMm/evOzatWsox+W/oQ6DZNeuXfXVq1fX\n6/V6/Y033qj/+te/HrB+6tSp+i233FI/ceJE/dNPP63/9Kc/rX/88cf967/97W/rd9xxR/3vf//7\nkM5NGRrdXzt37qyvWrWqXq/X6//+97/rCxcuHOrR+QF7880363fffXe9Xq/XDx48WL/rrrsGrP/k\nJz+pf/DBB/Vz587VlyxZUn/33XcveA58qZH91d3dXf/FL35Rr9fr9Y8++qg+Z86coR6bQjSyv770\nyCOP1H/2s5/Vn3/++SGdmcHnyhyDpru7O/Pnz0+SzJo1K/v27Ruw3tPTk+uvvz7Nzc255JJLMn36\n9P73dHd357LLLss111wz5HNThkb31+2335777rsvSdLS0pJjx44N+ez8cHV3d2fevHlJkqlTp+b4\n8ePp6+tLkvT29mb8+PGZOHFiRo0alTlz5qS7u/sbz4GvamR/zZw5M48++miSZNy4cfn0009z7ty5\nYfsO/HA1sr+S5NChQzl48GDmzp07XKMziMQcg6ZWq6WlpSVJMmrUqFQqlZw+ffq868kX/7CuVqs5\nffp0NmzYkHvvvXfIZ6Ycje6vMWPG5OKLL06SbN68ObfddtvQDs4PWq1Wy4QJE/qPv9w3SVKtVs+7\np77pHPiqRvbX6NGjc+mllyZJdu7cmdmzZ2f06NFDOzhFaGR/JcnatWuzYsWKoR2W/5qLhnsAyrRj\nx46vPfPW09Mz4Lher3/jZ3y5/uSTT6ajoyPjxo0b3CEp1mDury9t2bIlBw4cyMaNGwdnSP4nXWhf\nDdY5jEzfZa+89tpr2blzZ5555pn/4kT8L/k2++vFF1/MtGnTMnny5CGYiKEg5mhIR0dHOjo6Bry2\nYsWKVKvVXHvttTlz5kzq9Xqampr619va2lKr1fqPjx49mmnTpuWFF17I559/ni1btuT999/P/v37\n8+ijj+bqq68esu/DD8tg7q/kizh8/fXX8/jjj2fMmDFD8yUowvn2TWtr63nXjhw5kra2towZM+Y/\nngNf1cj+SpI33ngjGzduzNNPP53m5uahHZpiNLK/9uzZk97e3uzZsycffvhhmpqacuWVV2bWrFlD\nPj+Dw22WDJr29va88sorSZLdu3fn5ptvHrB+44035p133smJEydy6tSp7Nu3LzNmzMi2bduyffv2\nbN++PXPnzs2qVauEHF/T6P7q7e3Ntm3b8thjj/Xfbglfam9vz6uvvpokOXDgQNra2jJ27NgkyaRJ\nk9LX15fDhw/n7Nmz2b17d9rb27/xHPiqRvbXyZMns27dujzxxBO5/PLLh3N8fuAa2V9/+tOf8vzz\nz2f79u3p6OjIr371KyFXOFfmGDQLFizI3r17s2TJkjQ1NeXBBx9M8sVtlDNnzsxNN92Uzs7OLFu2\nLJVKJcuXL/c/jnxrje6vp556KseOHcvdd9/d/1mbNm0acFWPkWv69Om57rrrsnjx4lQqlaxatSq7\ndu1Kc3Nz5s+fn9WrV6ezszPJF3twypQpmTJlytfOgfNpZH91dXXl448/zm9+85v+z1m7dm2uuuqq\n4foa/EA1sr/431Opu9kfAACgOG6zBAAAKJCYAwAAKJCYAwAAKJCYAwAAKJCYAwAAKJCYAwAAKJCY\nAwAAKND/AXp9WANlcn53AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1080x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "0C-vQveSFXUZ",
        "colab_type": "code",
        "outputId": "fe686dd4-5a00-4bfa-a37d-c600cb50bcf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "preds = nn.predict(validate)\n",
        "calc_accuracy(labels_from_preds(preds), labels_from_preds(validate_target))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: RuntimeWarning: invalid value encountered in greater\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "metadata": {
        "id": "oGB6TzkSUpZE",
        "colab_type": "code",
        "outputId": "5cdf467b-f2fd-463b-e309-386f2f8e8340",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([[1,2], [3,4]])\n",
        "w = np.array([[1,2], [3,4]])\n",
        "d = np.array([1,2])\n",
        "print(np.dot(x, w)+d)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 8 12]\n",
            " [16 24]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hQyt7v35VPJs",
        "colab_type": "code",
        "outputId": "6fad152f-8009-4e32-a3a2-7addb829fcb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([1,2])\n",
        "w = np.array([[1,2], [3,4]])\n",
        "d = np.array([1,2])\n",
        "print(np.dot(x, w)+d)\n",
        "x = np.array([3,4])\n",
        "print(np.dot(x, w)+d)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 8 12]\n",
            "[16 24]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}