{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/desultir/SolrXLSXResponseWriter/blob/master/MNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "fQvPcbXfGAnt",
        "colab_type": "code",
        "outputId": "a2a639e7-5cd2-4dda-b7f9-9b30cbad29b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scipy\n",
        "!pip install --upgrade numpy\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: scipy in /usr/local/lib/python3.6/dist-packages (1.2.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.16.2)\n",
            "Requirement already up-to-date: numpy in /usr/local/lib/python3.6/dist-packages (1.16.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GGkj2RPTaJ9g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as pl\n",
        "from ipywidgets import interact, widgets\n",
        "from matplotlib import animation\n",
        "import h5py\n",
        "from google.colab import drive\n",
        "import time\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kf3k_uTGGlqK",
        "colab_type": "code",
        "outputId": "33a51fde-5813-4c9b-d5ff-9b9d557a04c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive', force_remount=True)\n",
        "with h5py.File('/content/drive/My Drive/Colab Notebooks/Input/train_128.h5','r') as H:\n",
        "  data = np.copy(H['data'])\n",
        "with h5py.File('/content/drive/My Drive/Colab Notebooks/Input/train_label.h5','r') as H:\n",
        "  label = np.copy(H['label'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rJ4v9obPcy13",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import pickle\n",
        "import glob\n",
        "\n",
        "model_dir = \"/content/drive/My Drive/Colab Notebooks/Models/{}.pk\"\n",
        "\n",
        "def list_models():\n",
        "  return glob.glob(model_dir.format(\"*\"))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DOSt_vG94Mfi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# need to normalize input data to avoid overflow/underflow in initial epochs\n",
        "# normalize each feature independently\n",
        "# options are zscore, minmax\n",
        "def preprocess(input_array, method='zscore'):\n",
        "  if method == 'zscore':\n",
        "    for i in range(input_array.shape[1]):\n",
        "      mean = np.mean(input_array[:, i])\n",
        "      std = np.std(input_array[:, i])\n",
        "      input_array[:, i] = (input_array[:, i] - mean) / std\n",
        "  elif method == 'minmax':\n",
        "    for i in range(input_array.shape[1]):\n",
        "      # range 0 to max\n",
        "      input_array[:, i] = (input_array[:, i] - np.min(input_array[:, i]))\n",
        "      # range 0 to 2\n",
        "      input_array[:, i] /= (np.max(input_array[:, i]) / 2)\n",
        "      # range -1 to 1\n",
        "      input_array[:, i] -= 1\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hCFYVtU06MPR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#use stratified sampling to split train into train/validation\n",
        "#this dataset is actually balanced but still good practice\n",
        "def split(dataset, labels, train_percent=.85):\n",
        "  count = len(dataset)\n",
        "  num_classes = np.max(label) + 1\n",
        "  train = []\n",
        "  train_target = []\n",
        "  validate = []\n",
        "  validate_target = []\n",
        "  for i in range(num_classes):\n",
        "    class_data = np.ravel(np.argwhere(label == i))\n",
        "    np.random.shuffle(class_data)\n",
        "    cutoff = int(len(class_data) * train_percent)\n",
        "    train_idx = class_data[:cutoff]\n",
        "    val_idx = class_data[cutoff:]\n",
        "    train.append(dataset[train_idx])\n",
        "    train_target.append(labels[train_idx])\n",
        "    validate.append(dataset[val_idx])\n",
        "    validate_target.append(labels[val_idx])\n",
        "    \n",
        "  return np.vstack(train), np.hstack(train_target), np.vstack(validate), np.hstack(validate_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RsZmOeyySQq9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#need to one-hot encode labels to map to N output nodes (1 per class)\n",
        "#ie convert each label into a (10,) vector where the relevant column is 1\n",
        "\n",
        "def OHE(input_array, num_classes=10):\n",
        "  output = []\n",
        "  for x in input_array:\n",
        "    output.append(np.zeros((10,)))\n",
        "    output[-1][x] = 1\n",
        "  return np.vstack(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q8KpB-EXchIh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## implemented formulae from here: https://towardsdatascience.com/hyper-parameters-in-action-part-ii-weight-initializers-35aee1a28404\n",
        "class InitWeights(object):\n",
        "  def xavier(self, n_in, n_out, uniform=True):\n",
        "    if uniform:\n",
        "      bounds = np.sqrt(6) / (np.sqrt(n_in + n_out))\n",
        "      return self._uniform(n_in, n_out, bounds) \n",
        "    else:\n",
        "      stddev = np.sqrt(2) / (np.sqrt(n_in + n_out))\n",
        "      return self._truncated_normal(n_in, n_out, stddev)\n",
        "    \n",
        "  def he(self, n_in, n_out, uniform=True):\n",
        "    if uniform:\n",
        "      bounds = np.sqrt(2) / (np.sqrt(n_in))\n",
        "      return self._uniform(n_in, n_out, bounds)      \n",
        "    else:\n",
        "      stddev = np.sqrt(6) / (np.sqrt(n_in))\n",
        "      return self._truncated_normal(n_in, n_out, stddev)\n",
        " \n",
        "  def _uniform(self, n_in, n_out, bounds):\n",
        "    W = np.random.uniform(\n",
        "        low=-bounds,\n",
        "        high=bounds,\n",
        "        size=(n_in, n_out)\n",
        "      )\n",
        "    return W\n",
        "  \n",
        "  def _truncated_normal(self, n_in, n_out, stddev):\n",
        "    W = np.random.normal(\n",
        "        loc=0,\n",
        "        scale=stddev,\n",
        "        size=(n_in, n_out)\n",
        "      )\n",
        "    #truncate results - anything > 2 stddev out gets clipped\n",
        "    W[W> 2*stddev] = 2*stddev\n",
        "    W[W<-2*stddev] = -2*stddev\n",
        "    return W\n",
        "  \n",
        "  def __init__(self, init_method=\"xavier\"):\n",
        "    if init_method==\"xavier\":\n",
        "      self.f = self.xavier\n",
        "    elif init_method==\"he\":\n",
        "      self.f = self.he"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "coc2QCMsn63E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def calc_MSE(y, y_hat):\n",
        "  error = y-y_hat\n",
        "  return np.mean(np.sum(error**2, axis=1))\n",
        "\n",
        "def labels_from_preds(preds):\n",
        "  return np.argmax(preds, axis=1)\n",
        "\n",
        "def calc_accuracy(labels, target):\n",
        "  return np.sum(labels == target) / len(target)\n",
        "\n",
        "#wasn't sure if we could use a package to shuffle so found this code: https://stackoverflow.com/questions/4601373/better-way-to-shuffle-two-numpy-arrays-in-unison\n",
        "def shuffle_in_unison(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    shuffled_a = np.empty(a.shape, dtype=a.dtype)\n",
        "    shuffled_b = np.empty(b.shape, dtype=b.dtype)\n",
        "    permutation = np.random.permutation(len(a))\n",
        "    for old_index, new_index in enumerate(permutation):\n",
        "        shuffled_a[new_index] = a[old_index]\n",
        "        shuffled_b[new_index] = b[old_index]\n",
        "    return shuffled_a, shuffled_b\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JbRDaYgyBsh8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The function for ReLU\n",
        "\n",
        "\n",
        "$ f(x) = \\begin{cases}\n",
        "    x & \\mbox{if } x > 0 \\\\\n",
        "    0 & \\mbox{otherwise}\n",
        "\\end{cases}$\n",
        "\n",
        "The function for ReLU's derivative\n",
        "\n",
        "$ f(x) = \\begin{cases}\n",
        "    1 & \\mbox{if } x > 0 \\\\\n",
        "    0 & \\mbox{otherwise}\n",
        "\\end{cases}$\n",
        "\n",
        "\n",
        "The function for Leaky ReLU\n",
        "\n",
        "\n",
        "$ f(x) = \\begin{cases}\n",
        "    x & \\mbox{if } x > 0 \\\\\n",
        "    0.01x & \\mbox{otherwise}\n",
        "\\end{cases}$\n",
        "\n",
        "The function for Leaky ReLU's derivative\n",
        "\n",
        "$ f(x) = \\begin{cases}\n",
        "    1 & \\mbox{if } x > 0 \\\\\n",
        "    0.01 & \\mbox{otherwise}\n",
        "\\end{cases}$\n"
      ]
    },
    {
      "metadata": {
        "id": "kYXDLyEWGG2r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "class Activation(object):\n",
        "    def tanh(self, x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def tanh_deriv(self, a):\n",
        "        # a = np.tanh(x)   \n",
        "        return 1.0 - a**2\n",
        "    def logistic(self, x):\n",
        "        return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "    def logistic_deriv(self, a):\n",
        "        # a = logistic(x) \n",
        "        return  a * (1 - a )\n",
        "      \n",
        "    def ReLU(self, x):\n",
        "        x[x<0] =0\n",
        "        return x\n",
        "      \n",
        "    def ReLU_deriv(self, a):\n",
        "        der = np.zeros(a.shape)\n",
        "        der[a>0] =1\n",
        "        return der\n",
        "      \n",
        "    def leaky_ReLU(self, x):\n",
        "        x = np.where(x > 0, x, x*0.01)\n",
        "        return x\n",
        "      \n",
        "    def leaky_ReLU_deriv(self, a):\n",
        "        der = np.full(a.shape, 0.01)\n",
        "        der[a>0] =1\n",
        "        return der\n",
        "      \n",
        "    def softmax(self, x):\n",
        "        # apply max normalization to avoid overflow\n",
        "        if len(x.shape) > 1:\n",
        "          x_norm = (x.T - np.max(x, axis=1)).T\n",
        "          return softmax(x_norm, axis=1)\n",
        "        else:\n",
        "          x_norm = x - np.max(x)\n",
        "          return softmax(x_norm)\n",
        "      \n",
        "    def softmax_deriv(self, a):\n",
        "        return np.ones(a.shape)\n",
        "    \n",
        "    def __init__(self,activation='tanh'):\n",
        "        if activation == 'logistic':\n",
        "            self.f = self.logistic\n",
        "            self.f_deriv = self.logistic_deriv\n",
        "        elif activation == 'tanh':\n",
        "            self.f = self.tanh\n",
        "            self.f_deriv = self.tanh_deriv\n",
        "        elif activation == 'relu':\n",
        "            self.f = self.ReLU\n",
        "            self.f_deriv = self.ReLU_deriv\n",
        "        elif activation == 'leaky_relu':\n",
        "            self.f = self.leaky_ReLU\n",
        "            self.f_deriv = self.leaky_ReLU_deriv\n",
        "        elif activation == 'softmax':\n",
        "            self.f = self.softmax\n",
        "            self.f_deriv = self.softmax_deriv\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HbjWqZ24L3Ot",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Loss_function(object):\n",
        "    def MSE(self, y, y_hat):\n",
        "        error = y-y_hat\n",
        "        loss=np.sum(error**2)\n",
        "        return loss\n",
        "      \n",
        "    def Cross_entropy(self, y, y_hat):\n",
        "        return -np.log(y_hat[np.argmax(y)])\n",
        "      \n",
        "    def l2_reg(self, reg_weight, layers, sample_weight):\n",
        "        accum = 0\n",
        "        for layer in layers:\n",
        "          accum += np.sum(np.square(layer.W))\n",
        "          \n",
        "        return accum*reg_weight*sample_weight/2\n",
        "        \n",
        "    def __init__(self,loss='cross_entropy'):\n",
        "        if loss == 'MSE':\n",
        "            self.loss = self.MSE\n",
        "        elif loss == 'cross_entropy':\n",
        "            self.loss = self.Cross_entropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BeoakGoCGLvb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "class HiddenLayer(object):    \n",
        "    def __init__(self,n_in, n_out,\n",
        "                 activation_last_layer='tanh',activation='tanh', W=None, b=None,\n",
        "                init_uniform=True, weight_decay=None, last_layer=False):\n",
        "        \"\"\"\n",
        "        Typical hidden layer of a MLP: units are fully-connected and have\n",
        "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
        "        and the bias vector b is of shape (n_out,).\n",
        "\n",
        "        NOTE : The nonlinearity used here is tanh\n",
        "\n",
        "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
        "\n",
        "        :type n_in: int\n",
        "        :param n_in: dimensionality of input\n",
        "\n",
        "        :type n_out: int\n",
        "        :param n_out: number of hidden units\n",
        "\n",
        "        :type activation: string\n",
        "        :param activation: Non linearity to be applied in the hidden\n",
        "                           layer\n",
        "        :type init_uniform: bool\n",
        "        :param init_uniform: Whether to draw init weights from uniform dist (else normal)\n",
        "        \n",
        "        :type weight_decay: float/None/False\n",
        "        :param weight_decay: Weight to apply to l2 reg loss factor (else none/false)\n",
        "        \"\"\"\n",
        "        self.input=None\n",
        "        self.dropout_cache=None\n",
        "        self.activation=Activation(activation).f\n",
        "        self.last_layer=last_layer\n",
        "        \n",
        "        if activation=='relu':\n",
        "          self.init_weights = InitWeights(\"he\").f\n",
        "        else:\n",
        "          self.init_weights = InitWeights(\"xavier\").f\n",
        "        \n",
        "        # activation deriv of last layer\n",
        "        self.activation_deriv=None\n",
        "        if activation_last_layer:\n",
        "            self.activation_deriv=Activation(activation_last_layer).f_deriv\n",
        "\n",
        "        if W is not None:\n",
        "          self.W = W\n",
        "        else:\n",
        "          self.W = self.init_weights(n_in, n_out, init_uniform)\n",
        "\n",
        "        if b is not None:\n",
        "          self.b = b\n",
        "        else:\n",
        "          self.b = np.zeros(n_out,)  \n",
        "          \n",
        "        self.weight_decay = weight_decay\n",
        "          \n",
        "        self.grad_W = np.zeros(self.W.shape)\n",
        "        self.grad_b = np.zeros(self.b.shape)\n",
        "        \n",
        "        # create arrays to store the velocity values for momentum calculation\n",
        "        self.vW = np.zeros(self.W.shape)\n",
        "        self.vb = np.zeros(self.b.shape)\n",
        "        \n",
        "        # Create arrays to store the gamma and beta values for batch norm. @\n",
        "        self.gamma = np.zeros(n_out,)\n",
        "        self.beta = np.zeros(n_out,)\n",
        "        \n",
        "        self.grad_gamma = np.zeros(self.gamma.shape,)\n",
        "        self.grad_beta = np.zeros(self.beta.shape,)\n",
        "    \n",
        "    def dropout(self, nodes, probability):\n",
        "        #This distribution decides what nodes will be on or not. We then rescale the on nodes proportionally to the probability that it is off.\n",
        "        \n",
        "        active_nodes = np.random.binomial(1, probability, size=nodes.shape) / probability\n",
        "        output = np.multiply(nodes, active_nodes)\n",
        "        return output, active_nodes\n",
        "    \n",
        "    def forward(self, input, probability):\n",
        "        '''\n",
        "        :type input: numpy.array\n",
        "        :param input: a symbolic tensor of shape (n_in,)\n",
        "        '''\n",
        "        lin_output = np.dot(input, self.W) + self.b\n",
        "        self.output = (\n",
        "            lin_output if self.activation is None\n",
        "            else self.activation(lin_output)\n",
        "        )\n",
        "        self.input=input\n",
        "        \n",
        "        if self.last_layer:\n",
        "            probability=1\n",
        "        \n",
        "        self.output, self.dropout_cache = self.dropout(self.output, probability)\n",
        "        return self.output\n",
        "    \n",
        "    def backward(self, delta, output_layer=False, sampleweight=1):\n",
        "        delta *= self.dropout_cache\n",
        "        self.grad_W = np.atleast_2d(self.input).T.dot(np.atleast_2d(delta))\n",
        "        \n",
        "        if self.weight_decay:\n",
        "            self.grad_W += self.W * self.weight_decay * sampleweight\n",
        "        self.grad_b = np.sum(delta, axis=0)\n",
        "        if self.activation_deriv:\n",
        "            delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
        "        return delta\n",
        "      \n",
        "    def forward_BN(self, input, probability):\n",
        "        '''\n",
        "        :type input: numpy.array\n",
        "        :param input: a symbolic tensor of shape (n_in,)\n",
        "        gamma: parameter to be learned\n",
        "        beta: parameter to be learned\n",
        "        '''\n",
        "        lin_output = np.dot(input, self.W) # Removed bias because it is forced to zero when normalizing ~(0,1) @\n",
        "        \n",
        "        # Can apply batch norm before or after activation function: https://www.reddit.com/r/MachineLearning/comments/67gonq/d_batch_normalization_before_or_after_relu/ @\n",
        "        mu = np.mean(lin_output, axis=0) # Calculate the mean of each feature.\n",
        "        var = np.var(lin_output, axis=0) # Calculat the variance of each feature.    \n",
        "                \n",
        "        lin_output = (lin_output - mu) / np.sqrt(var + 1e-8) # Normalise. Note the 1e-8 used incase var = 0.  \n",
        "        lin_output = self.gamma * lin_output + self.beta # Add gamma and beta so the mean and vairaince of distribution can be tuned.\n",
        "        \n",
        "        cache = (input, lin_output, mu, var, self.gamma, self.beta)  # Store values for back propogation.   \n",
        "                      \n",
        "        self.output = (\n",
        "            lin_output if self.activation is None\n",
        "            else self.activation(lin_output)\n",
        "        )\n",
        "        self.input=input # I'm not sure what this does? @\n",
        "        \n",
        "        if self.last_layer:\n",
        "            probability=1\n",
        "        \n",
        "        self.output, self.dropout_cache = self.dropout(self.output, probability)\n",
        "        return self.output, cache                               \n",
        "\n",
        "    def backward_BN(self, delta, dy_hat, cache, output_layer=False, sampleweight=1):\n",
        "      \n",
        "        # Unpack cache variables\n",
        "        X, X_norm, mu, var, gamma, beta = cache\n",
        "        \n",
        "        # Define variables to make back prop clearer\n",
        "        n_in, n_dims = X.shape\n",
        "        \n",
        "        X_mu = X - mu\n",
        "        std_inv = 1 / np.sqrt(var + 1e-8)\n",
        "        \n",
        "        # Back prop step by step for clarity. I followed this procedure: https://wiseodd.github.io/techblog/2016/07/04/batchnorm/\n",
        "        # Can also condense into a single calculation like this: https://cthorey.github.io/backpropagation/\n",
        "        dX_norm = dout * gamma\n",
        "        dvar = np.sum(dX_norm * X_mu, axis=0) * -0.5 * std_inv**3\n",
        "        dmu = np.sum(dx_norm * -std_inv, axis=0) + dvar * np.mean(-2 * X-mu, axis=0)\n",
        "        \n",
        "        self.grad_W = (dX_norm * std_inv) + (dvar * 2 * X_mu / n_in) + (dmu / n_in)\n",
        "        self.grad_gamma = np.sum(dy_hat * X_norm, axis=0)\n",
        "        self.grad_beta = np.sum(dy_hat, axis=0)            \n",
        "        \n",
        "        if self.weight_decay:\n",
        "          self.grad_W += self.W * self.weight_decay * sampleweight      \n",
        "        \n",
        "        if self.activation_deriv:\n",
        "            delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
        "        return delta\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_LKgiGhdGQbS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "class MLP:\n",
        "    \"\"\"\n",
        "    \"\"\"      \n",
        "    def __init__(self, layers=None, activation=[None,'tanh','tanh'], init_uniform=True, weight_decay=False, from_file=None):\n",
        "        \"\"\"\n",
        "        :param layers: A list containing the number of units in each layer.\n",
        "        Should be at least two values\n",
        "        :param activation: The activation function to be used. Can be\n",
        "        \"logistic\" or \"tanh\"\n",
        "        :param init_uniform: Whether to draw init weights from uniform dist (else normal)\n",
        "        :param weight_decay: lambda for strength of l2 regularization on weights (else False/None for no reg)\n",
        "        :param from_file: a file to load to get pretrained weights. \n",
        "        \"\"\"        \n",
        "        ### initialize layers\n",
        "        self.layers=[]\n",
        "        self.params= {'activation':activation, 'layers':layers, 'weight_decay': weight_decay, 'init_uniform': init_uniform}\n",
        "        \n",
        "        self.es_epochs=None\n",
        "        if from_file:\n",
        "          dumped_model = self._load_model(from_file)\n",
        "          self.params = dumped_model['params']\n",
        "          self.activation=self.params['activation']\n",
        "          layers = self.params['layers']\n",
        "          init_uniform = self.params['init_uniform']\n",
        "          for i in range(len(self.params['layers'])-1):\n",
        "              if i==len(self.params['layers'])-2:\n",
        "                self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1],  \n",
        "                                              W=dumped_model['weights'][i][0], b=dumped_model['weights'][i][1], weight_decay=weight_decay, init_uniform=init_uniform,last_layer=True))\n",
        "              else:\n",
        "                self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1],  \n",
        "                                              W=dumped_model['weights'][i][0], b=dumped_model['weights'][i][1], weight_decay=weight_decay, init_uniform=init_uniform))\n",
        "        else:\n",
        "          self.activation=activation\n",
        "          for i in range(len(layers)-1):\n",
        "              if i==len(layers)-2:\n",
        "                self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1], weight_decay=weight_decay, init_uniform=init_uniform,last_layer=True))\n",
        "              else:\n",
        "                self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1], weight_decay=weight_decay, init_uniform=init_uniform))\n",
        "    \n",
        "    def forward(self,input, dropout_p=1):\n",
        "        for layer in self.layers:\n",
        "            output=layer.forward(input, dropout_p)\n",
        "            input=output\n",
        "        return output\n",
        "    \n",
        "    #@ Can combine this with forward above with an if statement. Just wanted to keep separte for now until I know it works.\n",
        "    def Forward_BN(self,input, dropout_p=1):\n",
        "        for layer in self.layers:\n",
        "            output, cache=layer.forward_BN(input, dropout_p)\n",
        "            input=output\n",
        "        return output, cache\n",
        "      \n",
        "    def set_early_stopping(self, validation, validation_labels, num_epochs=10):\n",
        "        # for early stopping\n",
        "        self.validation = validation\n",
        "        self.validation_labels = labels_from_preds(validation_labels)\n",
        "        self.es_epochs = num_epochs\n",
        "      \n",
        "    def calculate_loss(self,y,y_hat):\n",
        "        activation_deriv=Activation(self.activation[-1]).f_deriv\n",
        "        # call to loss function\n",
        "        loss=[]\n",
        "        delta=[]\n",
        "        dy_hat=[] # store derivative of output for use in BN_backward @\n",
        "\n",
        "        for i, single_y in enumerate(y):\n",
        "          loss.append(Loss_function('MSE').loss(single_y, y_hat[i]))\n",
        "          error = single_y-y_hat[i]\n",
        "          # calculate the derivative of the output layer @\n",
        "          dy_hat.append(np.array(activation_deriv(y_hat[i])))\n",
        "          # calculate the delta of the output layer\n",
        "          delta.append(np.array(-error*activation_deriv(y_hat[i])))\n",
        "        # return loss and delta\n",
        "        loss = np.array(loss)\n",
        "        if self.params['weight_decay']:\n",
        "          loss += Loss_function().l2_reg(self.params['weight_decay'], self.layers, len(y)/self.Xcount)\n",
        "\n",
        "        return loss,np.array(delta), np.array(dy_hat) #@\n",
        "        \n",
        "    def backward(self,delta, sampleweight):\n",
        "        delta=self.layers[-1].backward(delta,output_layer=True, sampleweight=sampleweight)\n",
        "        for layer in reversed(self.layers[:-1]):\n",
        "            delta=layer.backward(delta, sampleweight=sampleweight)\n",
        "    \n",
        "    #@ can combine this with backwards above with an if statement. Just wanted to keep separte for now until I know it works.\n",
        "    def Backward_BN(self, delta, dy_hat, cache, sampleweight):\n",
        "        delta=self.layers[-1].backward_BN(delta, dy_hat, cache, output_layer=True, sampleweight=sampleweight)\n",
        "        for layer in reversed(self.layers[:-1]):\n",
        "            delta=layer.backward_BN(delta, sampleweight=sampleweight)            \n",
        "            \n",
        "    def update(self,lr):\n",
        "        for layer in self.layers:\n",
        "            layer.W -= lr * layer.grad_W\n",
        "            layer.b -= lr * layer.grad_b\n",
        "            \n",
        "    def update_momentum(self, lr, mom):\n",
        "        for layer in self.layers:\n",
        "            layer.vW = mom * layer.vW + lr * layer.grad_W\n",
        "            layer.vb = mom * layer.vb + lr * layer.grad_b\n",
        "            layer.W -= layer.vW\n",
        "            layer.b -= layer.vb        \n",
        "\n",
        "    def fit(self,X,y,learning_rate=0.1, epochs=100, dropout_p=1):\n",
        "        \"\"\"\n",
        "        Online learning.\n",
        "        :param X: Input data or features\n",
        "        :param y: Input targets\n",
        "        :param learning_rate: parameters defining the speed of learning\n",
        "        :param epochs: number of times the dataset is presented to the network for learning\n",
        "        \"\"\" \n",
        "        X=np.array(X)\n",
        "        y=np.array(y)\n",
        "        to_return = np.zeros(epochs)\n",
        "        self.Xcount = len(X)\n",
        "        if self.es_epochs:\n",
        "            validation_acc = np.zeros(epochs)\n",
        "        for k in range(epochs):\n",
        "            #print('epoch', k)\n",
        "            loss=np.zeros(X.shape[0])\n",
        "            for it in range(X.shape[0]):\n",
        "                i=np.random.randint(X.shape[0])\n",
        "                \n",
        "                # forward pass\n",
        "                y_hat = self.forward(X[i], dropout_p)\n",
        "                # backward pass\n",
        "                loss[it],delta=self.calculate_loss([y[i]],[y_hat])\n",
        "                self.backward(delta, 1/self.Xcount)\n",
        "                # update\n",
        "                self.update(learning_rate)\n",
        "            to_return[k] = np.mean(loss)\n",
        "            if not k % 10:\n",
        "              print(\".\", end=\"\")\n",
        "            if self.es_epochs:\n",
        "              preds = self.predict(self.validation)\n",
        "              validation_acc[k] = calc_accuracy(labels_from_preds(preds), self.validation_labels)\n",
        "              if k - np.argmax(validation_acc) > self.es_epochs:\n",
        "                print(\"Haven't improved accuracy on validation set in {} epochs, stopping\".format(self.es_epochs))\n",
        "                break\n",
        "        return to_return\n",
        "      \n",
        "    def fit_mb(self,X,y,mini_batch_size,learning_rate=0.1, epochs=100, dropout_p=1):\n",
        "        \"\"\"\n",
        "        Online learning.\n",
        "        :param X: Input data or features\n",
        "        :param y: Input targets\n",
        "        :param learning_rate: parameters defining the speed of learning\n",
        "        :param epochs: number of times the dataset is presented to the network for learning\n",
        "        :param early_stop: int: stop if haven't improved in this many epochs\n",
        "        \"\"\" \n",
        "        X=np.array(X)\n",
        "        y=np.array(y)\n",
        "        to_return = np.zeros(epochs) #array to store values of mean loss for each epoch for plotting later\n",
        "        if self.es_epochs:\n",
        "            validation_acc = np.zeros(epochs)\n",
        "        self.Xcount = len(X)\n",
        "        \n",
        "        for k in range(epochs): #for each epoch\n",
        "            X, y = shuffle_in_unison(X, y) #shuffle the input data and input targets\n",
        "            loss=np.zeros(X.shape[0]) #create array of zeros whose lengths = #samples.\n",
        "            \n",
        "            #partition training data (X, y) into mini-batches\n",
        "            for j in range(0, X.shape[0], mini_batch_size):\n",
        "              X_mini = X[j:j + mini_batch_size]\n",
        "              y_mini = y[j:j + mini_batch_size]\n",
        "              # forward pass\n",
        "              y_hat = self.forward(X_mini, dropout_p) #forward feed the mini_batches to get outputs (y_hat)\n",
        "              \n",
        "              # backwards pass\n",
        "              loss[j:j + mini_batch_size], delta=self.calculate_loss(y[j:j + mini_batch_size], y_hat) #input y and y_hat into calculate_loss. Output = loss and delta\n",
        "              self.backward(delta, mini_batch_size/self.Xcount) #pass delta from calculate_loss to backward.\n",
        "\n",
        "              # update\n",
        "              self.update(learning_rate)\n",
        "            to_return[k] = np.mean(loss) #add mean loss to to_return\n",
        "            if not k % 10:\n",
        "              print(\".\", end=\"\")\n",
        "            if self.es_epochs:\n",
        "              preds = self.predict(self.validation)\n",
        "              validation_acc[k] = calc_accuracy(labels_from_preds(preds), self.validation_labels)\n",
        "              if k - np.argmax(validation_acc) > self.es_epochs:\n",
        "                print(\"Haven't improved accuracy on validation set in {} epochs, stopping\".format(self.es_epochs))\n",
        "                break\n",
        "              \n",
        "        return to_return[:k]\n",
        "      \n",
        "    def fit_mb_BN(self,X,y,mini_batch_size,learning_rate=0.1, epochs=100, dropout_p=1):\n",
        "        \"\"\"\n",
        "        Online learning.\n",
        "        :param X: Input data or features\n",
        "        :param y: Input targets\n",
        "        :param learning_rate: parameters defining the speed of learning\n",
        "        :param epochs: number of times the dataset is presented to the network for learning\n",
        "        :param early_stop: int: stop if haven't improved in this many epochs\n",
        "        \"\"\" \n",
        "        X=np.array(X)\n",
        "        y=np.array(y)\n",
        "        to_return = np.zeros(epochs) #array to store values of mean loss for each epoch for plotting later\n",
        "        if self.es_epochs:\n",
        "            validation_acc = np.zeros(epochs)\n",
        "        self.Xcount = len(X)\n",
        "        \n",
        "        for k in range(epochs): #for each epoch\n",
        "            X, y = shuffle_in_unison(X, y) #shuffle the input data and input targets\n",
        "            loss=np.zeros(X.shape[0]) #create array of zeros whose lengths = #samples.\n",
        "            \n",
        "            #partition training data (X, y) into mini-batches\n",
        "            for j in range(0, X.shape[0], mini_batch_size):\n",
        "              X_mini = X[j:j + mini_batch_size]\n",
        "              y_mini = y[j:j + mini_batch_size]\n",
        "              # forward pass\n",
        "              y_hat, cache = self.Forward_BN(X_mini, dropout_p) #forward feed the mini_batches to get outputs (y_hat)\n",
        "              \n",
        "              # backwards pass\n",
        "              loss[j:j + mini_batch_size], delta, dy_hat=self.calculate_loss(y[j:j + mini_batch_size], y_hat) #input y and y_hat into calculate_loss. Output = loss and delta\n",
        "              self.Backward_BN(delta, dy_hat, cache, mini_batch_size/self.Xcount) #pass delta from calculate_loss to backward.\n",
        "\n",
        "              # update\n",
        "              self.update(learning_rate)\n",
        "            to_return[k] = np.mean(loss) #add mean loss to to_return\n",
        "            if not k % 10:\n",
        "              print(\".\", end=\"\")\n",
        "            if self.es_epochs:\n",
        "              preds = self.predict(self.validation)\n",
        "              validation_acc[k] = calc_accuracy(labels_from_preds(preds), self.validation_labels)\n",
        "              if k - np.argmax(validation_acc) > self.es_epochs:\n",
        "                print(\"Haven't improved accuracy on validation set in {} epochs, stopping\".format(self.es_epochs))\n",
        "                break\n",
        "              \n",
        "        return to_return[:k]\n",
        "      \n",
        "    def fit_SGD_momentum(self,X,y,learning_rate=0.1, epochs=100, momentum=0.9, dropout_p=1):\n",
        "        \"\"\"\n",
        "        Online learning.\n",
        "        :param X: Input data or features\n",
        "        :param y: Input targets\n",
        "        :param learning_rate: parameters defining the speed of learning\n",
        "        :param epochs: number of times the dataset is presented to the network for learning\n",
        "        \"\"\" \n",
        "        X=np.array(X)\n",
        "        y=np.array(y)\n",
        "        to_return = np.zeros(epochs)\n",
        "        self.Xcount = len(X)\n",
        "        if self.es_epochs:\n",
        "            validation_acc = np.zeros(epochs)\n",
        "        for k in range(epochs):\n",
        "            loss=np.zeros(X.shape[0])\n",
        "            \n",
        "            # loop through training examples\n",
        "            for j in range(X.shape[0]):\n",
        "              i=np.random.randint(X.shape[0])\n",
        "                \n",
        "              # forward pass\n",
        "              y_hat = self.forward(X[i], dropout_p)\n",
        "                \n",
        "              # backward pass\n",
        "              loss[j],delta=self.calculate_loss([y[i]],[y_hat])\n",
        "              self.backward(delta, X.shape[0]/self.Xcount)\n",
        "                \n",
        "              # update\n",
        "              self.update_momentum(learning_rate, momentum)\n",
        "            to_return[k] = np.mean(loss)\n",
        "            if not k % 10:\n",
        "              print(\".\", end=\"\")\n",
        "            if self.es_epochs:\n",
        "              preds = self.predict(self.validation)\n",
        "              validation_acc[k] = calc_accuracy(labels_from_preds(preds), self.validation_labels)\n",
        "              if k - np.argmax(validation_acc) > self.es_epochs:\n",
        "                print(\"Haven't improved accuracy on validation set in {} epochs, stopping\".format(self.es_epochs))\n",
        "                break\n",
        "        return to_return  \n",
        "\n",
        "    def predict(self, x):\n",
        "        x = np.array(x)\n",
        "        output = []\n",
        "        for i in np.arange(x.shape[0]):\n",
        "            output.append(self.forward(x[i,:]))\n",
        "        return np.vstack(output)\n",
        "      \n",
        "\n",
        "    def save_model(self, name):\n",
        "      model = {'params':self.params, 'weights':[]}\n",
        "      for x in self.layers:\n",
        "        model['weights'].append((x.W, x.b))\n",
        "        \n",
        "      with open(model_dir.format(name), 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "        \n",
        "    def _load_model(self, name):\n",
        "      with open(model_dir.format(name), 'rb') as f:\n",
        "        return pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DOPAt_9nGd5y",
        "colab_type": "code",
        "outputId": "e33e6d37-53a6-448e-bdee-0ea38575a5e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "np.seterr(all=\"warn\")\n",
        "np.random.seed(1)\n",
        "procdata = np.copy(data)\n",
        "preprocess(procdata, 'zscore')\n",
        "\n",
        "#split data\n",
        "train, train_target, validate, validate_target = split(procdata, label)\n",
        "#one hot encode targets\n",
        "train_target = OHE(train_target, 10)\n",
        "validate_target = OHE(validate_target, 10)\n",
        "second_layer = False\n",
        "relu = True\n",
        "if second_layer:\n",
        "  nn = MLP([128,60,30,10], [None,'logistic','logistic','tanh'])\n",
        "elif relu:\n",
        "  nn = MLP([128,60,10, 10], [None, 'relu','relu', 'softmax'])\n",
        "  #nn.set_early_stopping(validate, validate_target, 10)\n",
        "  start = time.time()\n",
        "  MSE = nn.fit_mb(train, train_target, learning_rate=0.001, epochs=20, mini_batch_size=32)\n",
        "  print(\"{}s to train\".format(time.time() - start))\n",
        "else:\n",
        "  nn = MLP([128,60,10], [None,'logistic','tanh'], init_uniform=False, weight_decay=0.5)\n",
        "  start = time.time()\n",
        "  MSE = nn.fit_mb(train, train_target, learning_rate=0.01, epochs=500, mini_batch_size=32)\n",
        "  print(\"{}s to train\".format(time.time() - start))\n",
        "print('loss:%f'%MSE[-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "..36.93035411834717s to train\n",
            "loss:0.112724\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-U-Uf7i9YX9t",
        "colab_type": "code",
        "outputId": "2b323197-616d-4f6d-e96c-5f2331a39f06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "cell_type": "code",
      "source": [
        "pl.figure(figsize=(15,4))\n",
        "pl.plot(MSE)\n",
        "pl.grid()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3QAAAD8CAYAAAA/m+aTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wd5TkiOJ0UQJAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1080x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "oAAnBZZ97anF",
        "colab_type": "code",
        "outputId": "c7a4a822-2f15-4cf1-c3ab-275f22642dff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8532222222222222"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "ms28tiW02uzz",
        "colab_type": "code",
        "outputId": "a85ce6f2-021a-4ab3-dfc0-85cda930b856",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "#get validation score\n",
        "#nn = load_model(\"tdm1\")\n",
        "preds = nn.predict(validate)\n",
        "\n",
        "loss = calc_MSE(preds, validate_target)\n",
        "loss"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: underflow encountered in square\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2558237762683418"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "Chank-E-im1p",
        "colab_type": "code",
        "outputId": "3fe1dcf8-b56f-45b5-ea92-7affcc4b2a6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "nn.save_model(\"relu.1\")\n",
        "\n",
        "calc_accuracy(labels_from_preds(preds), labels_from_preds(validate_target)) # we have to de-OHE the predictions and the target data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.856"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "VPIf-BuaBzlC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CauivsamljjX",
        "colab_type": "code",
        "outputId": "8913e40b-fb0d-4816-94a9-0e6ac6763f33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        }
      },
      "cell_type": "code",
      "source": [
        "np.seterr(all=\"warn\")\n",
        "np.random.seed(1)\n",
        "procdata = np.copy(data)\n",
        "preprocess(procdata, 'zscore')\n",
        "\n",
        "#split data\n",
        "train, train_target, validate, validate_target = split(data, label)\n",
        "#one hot encode targets\n",
        "train_target = OHE(train_target, 10)\n",
        "validate_target = OHE(validate_target, 10)\n",
        "second_layer = False\n",
        "relu = False\n",
        "if second_layer:\n",
        "  nn = MLP([128,60,30,10], [None,'logistic','logistic','tanh'])\n",
        "elif relu:\n",
        "  nn = MLP([128,60,10], [None, 'relu', 'relu'], False)\n",
        "  start = time.time()\n",
        "  MSE = nn.fit_mb_BN(train, train_target, 32, learning_rate=0.001, epochs=25, dropout_p=1)\n",
        "  print(\"{}s to train\".format(time.time() - start))\n",
        "else:\n",
        "  nn = MLP([128,60,10], [None,'logistic','tanh'], False)\n",
        "  start = time.time()\n",
        "  MSE = nn.fit_mb_BN(train, train_target, 32, learning_rate=0.001, epochs=25, dropout_p=1)\n",
        "  print(\"{}s to train\".format(time.time() - start))\n",
        "print('loss:%f'%MSE[-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-f3650aeeb622>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'logistic'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tanh'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mMSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_mb_BN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}s to train\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss:%f'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mMSE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-f6abd4d70f5f>\u001b[0m in \u001b[0;36mfit_mb_BN\u001b[0;34m(self, X, y, mini_batch_size, learning_rate, epochs, dropout_p)\u001b[0m\n\u001b[1;32m    214\u001b[0m               \u001b[0;31m# backwards pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m               \u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdy_hat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#input y and y_hat into calculate_loss. Output = loss and delta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBackward_BN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdy_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXcount\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#pass delta from calculate_loss to backward.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m               \u001b[0;31m# update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-f6abd4d70f5f>\u001b[0m in \u001b[0;36mBackward_BN\u001b[0;34m(self, delta, dy_hat, cache, sampleweight)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m#@ can combine this with backwards above with an if statement. Just wanted to keep separte for now until I know it works.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mBackward_BN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdy_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampleweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_BN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdy_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampleweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampleweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_BN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampleweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampleweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-eb2c8782e958>\u001b[0m in \u001b[0;36mbackward_BN\u001b[0;34m(self, delta, dy_hat, cache, output_layer, sampleweight)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mn_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mX_mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mstd_inv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (32,60) (10,) "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "hYPv5koXCwYk",
        "colab_type": "code",
        "outputId": "c2083867-0a08-4280-e885-c594b4dafdef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "len(train)\n",
        "len(validate)\n",
        "train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(51000, 128)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "XAmMCUzSupSo",
        "colab_type": "code",
        "outputId": "ea8c52e7-1e2b-45c6-9611-9cd221bffda7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "cell_type": "code",
      "source": [
        "pl.figure(figsize=(15,4))\n",
        "pl.plot(MSE)\n",
        "pl.grid()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3QAAAD8CAYAAAA/m+aTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZKkOmWgkyRJkqQ6ZaCTJEmSpDr1fygkVXA2I9gbAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1080x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "HExGAmdDalm0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Introduction\n",
        "The experiment task consisted of building a neural network to perform multi-class classification on a supplied dataset without the use of Deep Learning frameworks (e.g. TensorFlow, Caffe, and KERAS). The dataset consisted of 60,000 labeled training samples and 10,000 unlabeled test samples. The structure of the data (e.g. image, video, etc) was unknown. The performance of the neural network was evaluated in terms of the accuracy metric. Various neural network structures and parameters were trailed to maximise speed and accuracy.\n",
        "\n",
        "The objective of building the neural network without Deep Learning frameworks was to gain a comprehensive understanding of the math and mechanics behind neural networks.\n",
        "\n",
        "\n",
        "\n",
        "##SGD with Momentum\n",
        "Momentum ($v_t$) is an exponentially weighted average of a neural networks gradients. It is used to update the weights ($w_t$) and baises ($b_t$) of a network.\n",
        "\n",
        "$$v_t = \\beta v_{t-1} + \\eta \\nabla_w J(w)$$\n",
        "$$w_t = w_{t-1} - v_t$$\n",
        "\n",
        "Momentum increases for features whose gradients point in the same direction and reduces for features whose gradients change direction. By reducing the fluctuation of gradients convergence is generally sped up. The hyper-parameter $\\beta$ takes a value between 0 - 1 and dictates how many samples are included in the exponential weighted average. A small $\\beta$ value will increase fluctuation because the average is taken over a smaller number of examples. A large $\\beta$ will increase smoothing because the average is taken over a larger number of examples. A $\\beta$ value of 0.9 provides a balance between the two extremes.\n",
        "\n",
        "##Gradient Descent\n",
        "Gradient descent is a machine learning optimization method. In deep learning it is used to calculate the model parameters (weights and biases) that minimise the cost function. The gradient descent method invovles iterating through a training dataset and updating weights and baises in accordance with the gradient of error. There are three types of gradient descent. Each uses a different number of training examples to update the model parameters:\n",
        "*   **Batch Gradient Descent** uses the entire training dataset to calculate gradients and update the parameters. Because the entire training dataset is considered parameters updates are smooth however, it can take a long time to make a single update.\n",
        "*   **Stochastic Gradient Descent (SGD)** uses a single randomly selected sample from the training dataset to calculate gradients and update the parameters. Parameter updates are fast but very noisey.\n",
        "*   **Mini-batch Gradient Descent** uses a subset of the training data (e.g. batches of 1000 samples) to calculate gradients and update the parameters. Mini-batch gradient descent is a compromise between batch and stochastic gradient descent. The mini-batch size can be adjusted to find the appropriate balance between fast convergence and noisey updates. \n",
        "\n",
        "##Batch Normalization\n",
        "Batch Normalization is the normalization of a neural network's hidden layers so that the hidden units have standardised mean and variance. It is carried out on training data mini-batches, typically before the activation function is applied. For each mini-batch $(MB)$ the mean $(\\mu)$ and variance $(\\sigma^2)$ is calculated for all features:\n",
        "\n",
        "$$\\mu_{MB} = \\frac{1}{m} \\sum_{i = 1}^{m} x_i$$\n",
        "$$\\sigma_{MB}^2 \\frac{1}{m} \\sum_{i = 1}^{m} (x_i - \\mu_{MB})^2 $$\n",
        "\n",
        "The normalized values of the hidden unit $x_i$ can then be calculated:\n",
        "\n",
        "$$\\tilde{x_i} = \\frac{x_i - \\mu_{MB}}{\\sqrt{\\sigma_{MB}^2+\\epsilon}}$$\n",
        "\n",
        "$\\tilde{x_i}$ has a mean and variance of 0 and 1 respectively. It may be advantageous to alter the mean and variance of $\\tilde{x_i}$ to manipulate its distribution. Learnable hyperparameters $\\gamma$ and $\\beta$ are introduced to $\\tilde{x_i}$ for this purpose:\n",
        "$$\\tilde{x_{i}} = \\gamma\\tilde{x_i} + \\beta $$\n",
        "\n",
        "###Why does batch normalization work?\n",
        "Normalizing the input features of a neural network can speed up learning. This is becuase the gradient descent function can take larger steps due to the more symetic cost function. The same holds true from batch normalization. Learning can be sped up by normalising the values in the hidden units. The second reason that batch normalization can speed up training is that it makes weights deeper in the network more robust to chagnes that take place in earlier layers. The data distribution changes throughout the network layers, this is known as covariate shift. Batch normalization reduces the amount of covariate shift that occurs throughout the networks layers. Particuarly from the perpective of the later layers in the network the earlier layers don't shift around as much because they are constrianed to have the same mean and vairance. Good explanation video here: https://www.youtube.com/watch?v=nUUqwaxLnWs\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "BRDGNA5IPpGY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Louis testing stuff below"
      ]
    },
    {
      "metadata": {
        "id": "WLXWBGImOPVM",
        "colab_type": "code",
        "outputId": "184bf1f3-441d-439b-947e-c04b0df037f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "np.seterr(all=\"warn\")\n",
        "np.random.seed(1)\n",
        "procdata = np.copy(data)\n",
        "preprocess(procdata, 'zscore')\n",
        "\n",
        "#split data\n",
        "train, train_target, validate, validate_target = split(procdata, label)\n",
        "#one hot encode targets\n",
        "train_target = OHE(train_target, 10)\n",
        "validate_target = OHE(validate_target, 10)\n",
        "second_layer = False\n",
        "relu = True\n",
        "if second_layer:\n",
        "  nn = MLP([128,60,30,10], [None,'logistic','logistic','tanh'])\n",
        "elif relu:\n",
        "  nn = MLP([128,80,60, 30,10], [None, 'relu','relu','relu', 'softmax'])\n",
        "  start = time.time()\n",
        "  MSE = nn.fit_mb(train, train_target, learning_rate=0.001,mini_batch_size=32, epochs=500, dropout_p=0.5)\n",
        "  print(\"{}s to train\".format(time.time() - start))\n",
        "else:\n",
        "  nn = MLP([128,60,10], [None,'logistic','tanh'], init_uniform=False, weight_decay=0.5)\n",
        "  start = time.time()\n",
        "  MSE = nn.fit_mb(train, train_target, learning_rate=0.01, epochs=500, mini_batch_size=32)\n",
        "  print(\"{}s to train\".format(time.time() - start))\n",
        "print('loss:%f'%MSE[-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: underflow encountered in square\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ".."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/special/_logsumexp.py:112: RuntimeWarning: underflow encountered in exp\n",
            "  tmp = np.exp(a - a_max)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/special/_logsumexp.py:215: RuntimeWarning: underflow encountered in exp\n",
            "  return np.exp(x - logsumexp(x, axis=axis, keepdims=True))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "............"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:71: RuntimeWarning: underflow encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "......................."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:72: RuntimeWarning: underflow encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "............1733.7815234661102s to train\n",
            "loss:0.228955\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0C-vQveSFXUZ",
        "colab_type": "code",
        "outputId": "4306b888-bd7d-4ac9-a6d9-015b22784427",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "preds = nn.predict(validate)\n",
        "calc_accuracy(labels_from_preds(preds), labels_from_preds(validate_target))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8802222222222222"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "metadata": {
        "id": "6yXjnNSj_bv3",
        "colab_type": "code",
        "outputId": "f7246b70-7133-40c8-d952-84754603e524",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "cell_type": "code",
      "source": [
        "pl.figure(figsize=(15,4))\n",
        "pl.plot(MSE)\n",
        "pl.grid()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA20AAAD8CAYAAADkIEyxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niYiIiIiIZLD/H4Pykoike9SIAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1080x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}